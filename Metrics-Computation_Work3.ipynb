{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5aa9e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "### This script computes various network metrics and provides Visualization from CSV files containing\n",
    "### weighted adjacency matrices with coordinates. \n",
    "### DEBUGGED AND EDITED\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a2c8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "import community.community_louvain as community_louvain\n",
    "import community as community_louvain\n",
    "import os, glob\n",
    "\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218aaf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "# Define the directory and file patterns\n",
    "# Adjust the paths as necessary for your environment\n",
    "#############################################################################################\n",
    "\n",
    "data_dir = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\"\n",
    "# file_path =  r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation5\\weighted_matrix_combined_tick_338.csv\"\n",
    "# file_path =  r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation5\\weighted_matrix_with_coords_combined_tick_338.csv\"\n",
    "\n",
    "file_path = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\\weighted_matrix_combined_tick_338.csv\"\n",
    "\n",
    "data_dir = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\"\n",
    "pattern     = os.path.join(data_dir, \"weighted_matrix_xy_combined_tick_*.csv\")\n",
    "output_dir = os.path.join(data_dir, \"Metrics_Results\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "file_path = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\\weighted_matrix_with_coords_combined_tick_182.csv\"\n",
    "\n",
    "pattern = os.path.join(data_dir, \"weighted_matrix_with_coords_combined_tick_*.csv\")\n",
    "files = glob.glob(pattern)\n",
    "# sort by tick-number at end of filename\n",
    "files = sorted(files, key=lambda fp: int(os.path.basename(fp).split('_')[-1].split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e99480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Load network from CSV file\n",
    "############################################################################################\n",
    "\n",
    "# Load network from CSV file\n",
    "def load_network_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Load network data from CSV file into a NetworkX graph\n",
    "    \"\"\"\n",
    "    print(f\"Loading network from: {file_path}\")\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded data with {len(df)} rows\")\n",
    "    \n",
    "    # Display first few rows to verify structure\n",
    "    print(\"\\nFirst few rows of data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Create an empty directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes and edges to the graph\n",
    "    for _, row in df.iterrows():\n",
    "        source = row['source']\n",
    "        target = row['target']\n",
    "        weight = row['weight']\n",
    "        \n",
    "        # Add nodes if they don't exist\n",
    "        if not G.has_node(source):\n",
    "            G.add_node(source)\n",
    "        if not G.has_node(target):\n",
    "            G.add_node(target)\n",
    "        \n",
    "        # Add edge with weight\n",
    "        G.add_edge(source, target, weight=weight)\n",
    "    \n",
    "    print(f\"Created graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbc48ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Load network from CSV file\n",
    "############################################################################################\n",
    "\n",
    "def load_network_csv(file_path):\n",
    "    \"\"\"\n",
    "    Load network data from CSV file into a NetworkX graph,\n",
    "    dropping any rows whose weight cannot be parsed as a float.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # coerce weight to numeric, setting invalid parses to NaN\n",
    "    df['weight'] = pd.to_numeric(df['weight'], errors='coerce')\n",
    "\n",
    "    # drop any rows where source/target aren't integers or weight is NaN\n",
    "    df = df.dropna(subset=['source','target','weight'])\n",
    "    df['source'] = df['source'].astype(int)\n",
    "    df['target'] = df['target'].astype(int)\n",
    "    df['weight'] = df['weight'].astype(float)\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in df.iterrows():\n",
    "        G.add_edge(row['source'],\n",
    "                   row['target'],\n",
    "                   weight=row['weight'],\n",
    "                   label=row.get('label',None))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "393a8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Analyze direct interactions (1-hop)\n",
    "############################################################################################\n",
    "\n",
    "def analyze_direct_and_indirect_interactions(G):\n",
    "    \"\"\"\n",
    "    Analyze direct (1-hop) and indirect (2-hop) interactions in an ecological network\n",
    "    where weights represent interaction strength (higher = stronger interaction)\n",
    "    and signs represent interaction type (positive = synergy, negative = antagonism)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Basic network properties\n",
    "    results['num_nodes'] = G.number_of_nodes()\n",
    "    results['num_edges'] = G.number_of_edges()\n",
    "    results['density'] = nx.density(G)\n",
    "    \n",
    "    # 2. Direct interaction analysis\n",
    "    direct_interactions = analyze_direct_interactions(G)\n",
    "    results.update(direct_interactions)\n",
    "    \n",
    "    # 3. Indirect interaction analysis (2-hop paths)\n",
    "    indirect_interactions = analyze_indirect_interactions(G)\n",
    "    results.update(indirect_interactions)\n",
    "    \n",
    "    # 4. Node-level metrics\n",
    "    node_metrics = compute_node_level_metrics(G)\n",
    "    results['node_metrics'] = node_metrics\n",
    "    \n",
    "    # 5. Community detection based on interaction patterns\n",
    "    communities = detect_interaction_communities(G)\n",
    "    results['communities'] = communities\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2255e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Analyze direct interactions (1-hop)\n",
    "############################################################################################\n",
    "\n",
    "def analyze_direct_interactions(G):\n",
    "    \"\"\"\n",
    "    Analyze direct (1-hop) interactions between species\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Count positive and negative interactions\n",
    "    pos_edges = [(u, v) for u, v, w in G.edges(data='weight') if w > 0]\n",
    "    neg_edges = [(u, v) for u, v, w in G.edges(data='weight') if w < 0]\n",
    "    \n",
    "    metrics['positive_edges'] = len(pos_edges)\n",
    "    metrics['negative_edges'] = len(neg_edges)\n",
    "    metrics['pos_neg_ratio'] = len(pos_edges) / len(neg_edges) if len(neg_edges) > 0 else float('inf')\n",
    "    \n",
    "    # Calculate average strength of positive and negative interactions\n",
    "    pos_weights = [G[u][v]['weight'] for u, v in pos_edges]\n",
    "    neg_weights = [abs(G[u][v]['weight']) for u, v in neg_edges]\n",
    "    \n",
    "    metrics['avg_positive_strength'] = np.mean(pos_weights) if pos_weights else 0\n",
    "    metrics['avg_negative_strength'] = np.mean(neg_weights) if neg_weights else 0\n",
    "    metrics['max_positive_strength'] = max(pos_weights) if pos_weights else 0\n",
    "    metrics['max_negative_strength'] = max(neg_weights) if neg_weights else 0\n",
    "    \n",
    "    # Calculate reciprocity (proportion of interactions that are reciprocated)\n",
    "    metrics['reciprocity'] = nx.reciprocity(G)\n",
    "    \n",
    "    # Calculate asymmetry in interactions\n",
    "    asymmetry_scores = []\n",
    "    for u, v in G.edges():\n",
    "        if G.has_edge(v, u):\n",
    "            w1 = G[u][v]['weight']\n",
    "            w2 = G[v][u]['weight']\n",
    "            # Measure both strength asymmetry and sign asymmetry\n",
    "            strength_asymmetry = abs(abs(w1) - abs(w2)) / (abs(w1) + abs(w2)) if (abs(w1) + abs(w2)) > 0 else 0\n",
    "            sign_different = (w1 * w2) < 0  # True if signs are different\n",
    "            asymmetry_scores.append((strength_asymmetry, sign_different))\n",
    "    \n",
    "    if asymmetry_scores:\n",
    "        metrics['avg_strength_asymmetry'] = np.mean([a[0] for a in asymmetry_scores])\n",
    "        metrics['prop_sign_differences'] = np.mean([int(a[1]) for a in asymmetry_scores])\n",
    "    else:\n",
    "        metrics['avg_strength_asymmetry'] = 0\n",
    "        metrics['prop_sign_differences'] = 0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6ca7029",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Analyze indirect interactions (1-hop)\n",
    "############################################################################################\n",
    "\n",
    "def analyze_indirect_interactions(G):\n",
    "    \"\"\"\n",
    "    Analyze indirect (2-hop) interactions and their relationship to direct interactions\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Store all 2-hop paths and their compound effect\n",
    "    two_hop_paths = []\n",
    "    two_hop_effects = []\n",
    "    two_hop_connections = defaultdict(list)\n",
    "    \n",
    "    # For each pair of nodes\n",
    "    for source in G.nodes():\n",
    "        for target in G.nodes():\n",
    "            if source != target:\n",
    "                # Check if there's a direct connection\n",
    "                direct_effect = G[source][target]['weight'] if G.has_edge(source, target) else 0\n",
    "                \n",
    "                # Find all 2-hop paths (through one intermediate node)\n",
    "                indirect_paths = []\n",
    "                for intermediate in G.successors(source):\n",
    "                    if G.has_edge(intermediate, target) and intermediate != target:\n",
    "                        weight1 = G[source][intermediate]['weight']\n",
    "                        weight2 = G[intermediate][target]['weight']\n",
    "                        compound_effect = weight1 * weight2  # Multiplication preserves sign logic\n",
    "                        \n",
    "                        indirect_paths.append({\n",
    "                            'path': (source, intermediate, target),\n",
    "                            'effect': compound_effect,\n",
    "                            'weights': (weight1, weight2)\n",
    "                        })\n",
    "                \n",
    "                if indirect_paths:\n",
    "                    # Store all indirect paths\n",
    "                    two_hop_paths.extend(indirect_paths)\n",
    "                    \n",
    "                    # Calculate total indirect effect (sum of all 2-hop paths)\n",
    "                    total_indirect_effect = sum(p['effect'] for p in indirect_paths)\n",
    "                    two_hop_effects.append(total_indirect_effect)\n",
    "                    \n",
    "                    # Compare direct and indirect effects if both exist\n",
    "                    if direct_effect != 0:\n",
    "                        two_hop_connections[(source, target)].append({\n",
    "                            'direct_effect': direct_effect,\n",
    "                            'indirect_effect': total_indirect_effect,\n",
    "                            'indirect_paths': indirect_paths\n",
    "                        })\n",
    "    \n",
    "    # Analyze the distribution of 2-hop effects\n",
    "    if two_hop_effects:\n",
    "        metrics['num_2hop_paths'] = len(two_hop_paths)\n",
    "        metrics['avg_2hop_effect'] = np.mean(two_hop_effects)\n",
    "        metrics['max_2hop_effect'] = max(two_hop_effects)\n",
    "        metrics['min_2hop_effect'] = min(two_hop_effects)\n",
    "        \n",
    "        # Count positive and negative 2-hop effects\n",
    "        pos_2hop = sum(1 for e in two_hop_effects if e > 0)\n",
    "        neg_2hop = sum(1 for e in two_hop_effects if e < 0)\n",
    "        metrics['positive_2hop'] = pos_2hop\n",
    "        metrics['negative_2hop'] = neg_2hop\n",
    "        metrics['pos_neg_2hop_ratio'] = pos_2hop / neg_2hop if neg_2hop > 0 else float('inf')\n",
    "    \n",
    "    # Analyze reinforcing vs. counteracting effects\n",
    "    if two_hop_connections:\n",
    "        reinforcing = 0\n",
    "        counteracting = 0\n",
    "        total = 0\n",
    "        \n",
    "        for (source, target), connections in two_hop_connections.items():\n",
    "            for conn in connections:\n",
    "                direct = conn['direct_effect']\n",
    "                indirect = conn['indirect_effect']\n",
    "                \n",
    "                if direct * indirect > 0:  # Same sign - reinforcing\n",
    "                    reinforcing += 1\n",
    "                else:  # Different sign - counteracting\n",
    "                    counteracting += 1\n",
    "                total += 1\n",
    "        \n",
    "        metrics['reinforcing_effects'] = reinforcing\n",
    "        metrics['counteracting_effects'] = counteracting\n",
    "        metrics['reinforcing_ratio'] = reinforcing / total if total > 0 else 0\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed1baad",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Compute node-level metrics\n",
    "############################################################################################\n",
    "\n",
    "def ecological_closeness_centrality(G):\n",
    "    \"\"\"\n",
    "    Custom ecological closeness centrality that:\n",
    "    1. Treats stronger interactions as \"closer\"\n",
    "    2. Preserves sign information \n",
    "    3. Only considers 1-hop and 2-hop neighbors\n",
    "    \"\"\"\n",
    "    closeness_scores = {}\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        # Direct interactions (1-hop)\n",
    "        direct_neighbors = list(G.successors(node))\n",
    "        direct_influences = {}\n",
    "        \n",
    "        for neighbor in direct_neighbors:\n",
    "            weight = G[node][neighbor]['weight']\n",
    "            # Transform weight: stronger interactions (higher |weight|) mean closer ecological distance\n",
    "            # We use absolute value but preserve sign with signum function\n",
    "            influence = np.sign(weight) * (1.0 / (1.0 + np.exp(-abs(weight))))\n",
    "            direct_influences[neighbor] = influence\n",
    "        \n",
    "        # Indirect interactions (2-hop)\n",
    "        indirect_influences = {}\n",
    "        for direct_neighbor in direct_neighbors:\n",
    "            for indirect_neighbor in G.successors(direct_neighbor):\n",
    "                if indirect_neighbor == node or indirect_neighbor in direct_neighbors:\n",
    "                    continue  # Skip self-loops and direct neighbors\n",
    "                \n",
    "                weight1 = G[node][direct_neighbor]['weight']\n",
    "                weight2 = G[direct_neighbor][indirect_neighbor]['weight']\n",
    "                \n",
    "                # Compound effect with attenuation\n",
    "                compound_effect = np.sign(weight1 * weight2) * (abs(weight1 * weight2) ** 0.5)\n",
    "                \n",
    "                # Dampen effect based on path length (2-hop gets 50% weight)\n",
    "                influence = 0.5 * np.sign(compound_effect) * (1.0 / (1.0 + np.exp(-abs(compound_effect))))\n",
    "                \n",
    "                # Add to existing influence if there are multiple paths\n",
    "                if indirect_neighbor in indirect_influences:\n",
    "                    indirect_influences[indirect_neighbor] += influence\n",
    "                else:\n",
    "                    indirect_influences[indirect_neighbor] = influence\n",
    "        \n",
    "        # Calculate positive and negative influence reaches\n",
    "        pos_direct = sum(infl for infl in direct_influences.values() if infl > 0)\n",
    "        neg_direct = sum(abs(infl) for infl in direct_influences.values() if infl < 0)\n",
    "        pos_indirect = sum(infl for infl in indirect_influences.values() if infl > 0)\n",
    "        neg_indirect = sum(abs(infl) for infl in indirect_influences.values() if infl < 0)\n",
    "        \n",
    "        # Calculate total ecological influence (closeness)\n",
    "        total_positive = pos_direct + pos_indirect\n",
    "        total_negative = neg_direct + neg_indirect\n",
    "        total_influence = total_positive + total_negative\n",
    "        net_influence = total_positive - total_negative\n",
    "        \n",
    "        closeness_scores[node] = {\n",
    "            'positive_influence': total_positive,\n",
    "            'negative_influence': total_negative,\n",
    "            'total_influence': total_influence,\n",
    "            'net_influence': net_influence,\n",
    "            'direct_reach': len(direct_influences),\n",
    "            'indirect_reach': len(indirect_influences),\n",
    "            'total_reach': len(direct_influences) + len(indirect_influences)\n",
    "        }\n",
    "    \n",
    "    return closeness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "612390e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Compute node-level metrics\n",
    "############################################################################################\n",
    "\n",
    "def compute_node_level_metrics(G):\n",
    "    \"\"\"\n",
    "    Compute various node-level ecological metrics\n",
    "    \"\"\"\n",
    "    node_metrics = {}\n",
    "    \n",
    "    # Calculate ecological closeness centrality\n",
    "    ecological_closeness = ecological_closeness_centrality(G)\n",
    "    node_metrics['ecological_closeness'] = ecological_closeness\n",
    "    \n",
    "    # Calculate number of positive and negative connections per node\n",
    "    for node in G.nodes():\n",
    "        pos_out = sum(1 for _, v, w in G.out_edges(node, data='weight') if w > 0)\n",
    "        neg_out = sum(1 for _, v, w in G.out_edges(node, data='weight') if w < 0)\n",
    "        pos_in = sum(1 for u, _, w in G.in_edges(node, data='weight') if w > 0)\n",
    "        neg_in = sum(1 for u, _, w in G.in_edges(node, data='weight') if w < 0)\n",
    "        \n",
    "        if node not in node_metrics:\n",
    "            node_metrics[node] = {}\n",
    "            \n",
    "        node_metrics[node]['positive_outgoing'] = pos_out\n",
    "        node_metrics[node]['negative_outgoing'] = neg_out\n",
    "        node_metrics[node]['positive_incoming'] = pos_in\n",
    "        node_metrics[node]['negative_incoming'] = neg_in\n",
    "        node_metrics[node]['net_positive'] = (pos_out + pos_in) - (neg_out + neg_in)\n",
    "    \n",
    "    # Identify keystone species (high betweenness in 2-hop paths)\n",
    "    keystone_scores = {}\n",
    "    for node in G.nodes():\n",
    "        # Count how many 2-hop paths go through this node\n",
    "        path_count = 0\n",
    "        for source in G.nodes():\n",
    "            if source == node:\n",
    "                continue\n",
    "            for target in G.nodes():\n",
    "                if target == node or target == source:\n",
    "                    continue\n",
    "                if G.has_edge(source, node) and G.has_edge(node, target):\n",
    "                    path_count += 1\n",
    "        \n",
    "        keystone_scores[node] = path_count\n",
    "    \n",
    "    # Normalize keystone scores\n",
    "    max_score = max(keystone_scores.values()) if keystone_scores else 1\n",
    "    for node, score in keystone_scores.items():\n",
    "        if node not in node_metrics:\n",
    "            node_metrics[node] = {}\n",
    "        node_metrics[node]['keystone_score'] = score / max_score if max_score > 0 else 0\n",
    "    \n",
    "    return node_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3953f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Detect communities based on interaction patterns\n",
    "############################################################################################\n",
    "\n",
    "def detect_interaction_communities(G):\n",
    "    \"\"\"\n",
    "    Detect communities based on interaction patterns\n",
    "    \"\"\"\n",
    "    # Create a modified graph for community detection where:\n",
    "    # - Edge weights are converted to represent similarity (higher = more similar)\n",
    "    # - Negative edges represent dissimilarity\n",
    "    G_comm = nx.Graph()\n",
    "    \n",
    "    for u, v, data in G.edges(data=True):\n",
    "        weight = data['weight']\n",
    "        # Convert weight to similarity measure: |weight| represents strength, sign preserved\n",
    "        if weight > 0:  # Positive interaction = similarity\n",
    "            sim_weight = weight\n",
    "        else:  # Negative interaction = dissimilarity\n",
    "            sim_weight = weight  # Keep negative to represent dissimilarity\n",
    "        \n",
    "        if G_comm.has_edge(u, v):\n",
    "            # If edge exists, add to current weight (accumulate evidence)\n",
    "            G_comm[u][v]['weight'] += sim_weight\n",
    "        else:\n",
    "            G_comm.add_edge(u, v, weight=sim_weight)\n",
    "    \n",
    "    # Create a positive-only graph for community detection algorithms that require it\n",
    "    G_pos = nx.Graph()\n",
    "    for u, v, data in G_comm.edges(data=True):\n",
    "        # Take absolute weight but mark edges that were negative\n",
    "        weight = abs(data['weight'])\n",
    "        is_negative = data['weight'] < 0\n",
    "        G_pos.add_edge(u, v, weight=weight, is_negative=is_negative)\n",
    "    \n",
    "    # Detect communities using Louvain method\n",
    "    try:\n",
    "        # Check if python-louvain is installed\n",
    "        import community as community_louvain\n",
    "        partition = community_louvain.best_partition(G_pos, weight='weight')\n",
    "        \n",
    "        # Convert partition format\n",
    "        communities = defaultdict(list)\n",
    "        for node, community_id in partition.items():\n",
    "            communities[community_id].append(node)\n",
    "        \n",
    "        return {\n",
    "            'algorithm': 'louvain',\n",
    "            'communities': dict(communities),\n",
    "            'num_communities': len(communities)\n",
    "        }\n",
    "    except ImportError:\n",
    "        # Fall back to NetworkX's community detection if python-louvain is not available\n",
    "        try:\n",
    "            from networkx.algorithms import community\n",
    "            communities_generator = community.greedy_modularity_communities(G_pos, weight='weight')\n",
    "            communities = {i: list(comm) for i, comm in enumerate(communities_generator)}\n",
    "            \n",
    "            return {\n",
    "                'algorithm': 'greedy_modularity',\n",
    "                'communities': communities,\n",
    "                'num_communities': len(communities)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'algorithm': 'failed',\n",
    "                'error': str(e),\n",
    "                'communities': {},\n",
    "                'num_communities': 0\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d2472d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################    \n",
    "# Visualize the ecological network using actual spatial coordinates\n",
    "############################################################################################\n",
    "\n",
    "def visualize_spatial_network(G, node_coords, node_metrics=None, communities=None, \n",
    "                             title=\"Spatial Ecological Network\", pos_color='blue', \n",
    "                             neg_color='red', max_edges=2000, node_size_factor=100,\n",
    "                             edge_width_factor=1.0, show_labels=False):\n",
    "    \"\"\"\n",
    "    Visualize the ecological network using actual spatial coordinates\n",
    "    \"\"\"\n",
    "    # Create figure and axis explicitly\n",
    "    fig, ax = plt.subplots(figsize=(16, 16))\n",
    "    \n",
    "    # Use the actual spatial coordinates for node positions\n",
    "    pos = node_coords\n",
    "    \n",
    "    # Prepare node sizes based on metrics or default size\n",
    "    if node_metrics:\n",
    "        node_sizes = []\n",
    "        for node in G.nodes():\n",
    "            if node in node_metrics and 'ecological_closeness' in node_metrics[node]:\n",
    "                # Size based on total influence\n",
    "                influence = node_metrics[node]['ecological_closeness'].get('total_influence', 0)\n",
    "                size = node_size_factor * (1 + influence)\n",
    "            else:\n",
    "                # Fallback size\n",
    "                size = node_size_factor\n",
    "            node_sizes.append(size)\n",
    "    else:\n",
    "        # Default node size\n",
    "        node_sizes = [node_size_factor] * G.number_of_nodes()\n",
    "    \n",
    "    # Prepare node colors based on communities\n",
    "    if communities and 'communities' in communities and communities['communities']:\n",
    "        # Create a reverse mapping from node to community ID\n",
    "        node_community = {}\n",
    "        for comm_id, nodes in communities['communities'].items():\n",
    "            for node in nodes:\n",
    "                node_community[node] = int(comm_id)\n",
    "        \n",
    "        # Generate colors for nodes\n",
    "        node_colors = [node_community.get(node, 0) for node in G.nodes()]\n",
    "        # Use a qualitative colormap\n",
    "        import matplotlib as mpl\n",
    "        max_community = max(node_community.values()) if node_community else 0\n",
    "        cmap = mpl.colormaps.get_cmap('tab20')\n",
    "        norm = mpl.colors.Normalize(vmin=0, vmax=max_community)\n",
    "    else:\n",
    "        # Default coloring - GREEN nodes as requested\n",
    "        node_colors = 'green'\n",
    "        cmap = None\n",
    "        norm = None\n",
    "    \n",
    "    # Separate positive and negative edges\n",
    "    pos_edges = [(u, v) for u, v, data in G.edges(data=True) if data['weight'] > 0]\n",
    "    neg_edges = [(u, v) for u, v, data in G.edges(data=True) if data['weight'] < 0]\n",
    "    \n",
    "    # Limit the number of edges for large networks\n",
    "    if len(pos_edges) > max_edges:\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        pos_edges = random.sample(pos_edges, max_edges)\n",
    "    if len(neg_edges) > max_edges:\n",
    "        import random\n",
    "        random.seed(42)\n",
    "        neg_edges = random.sample(neg_edges, max_edges)\n",
    "    \n",
    "    # Calculate edge widths based on absolute weight\n",
    "    pos_weights = [edge_width_factor * abs(G[u][v]['weight']) for u, v in pos_edges]\n",
    "    neg_weights = [edge_width_factor * abs(G[u][v]['weight']) for u, v in neg_edges]\n",
    "    \n",
    "    # Draw nodes - GREEN as requested\n",
    "    nodes = nx.draw_networkx_nodes(G, pos, ax=ax, node_size=node_sizes, node_color=node_colors, \n",
    "                                  alpha=0.7, cmap=cmap)\n",
    "    \n",
    "    # Draw edges - BLUE for positive, RED for negative as requested\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, edgelist=pos_edges, width=pos_weights, \n",
    "                          edge_color=pos_color, alpha=0.6, arrows=True, arrowsize=10)\n",
    "    nx.draw_networkx_edges(G, pos, ax=ax, edgelist=neg_edges, width=neg_weights, \n",
    "                          edge_color=neg_color, alpha=0.6, arrows=True, \n",
    "                          arrowstyle='-|>', arrowsize=15)\n",
    "    \n",
    "    # Add labels if requested\n",
    "    if show_labels:\n",
    "        # Only label nodes with significant importance\n",
    "        if node_metrics:\n",
    "            important_nodes = {}\n",
    "            for node in G.nodes():\n",
    "                if (node in node_metrics and \n",
    "                    'ecological_closeness' in node_metrics[node] and\n",
    "                    node_metrics[node]['ecological_closeness'].get('total_influence', 0) > 0.2):\n",
    "                    important_nodes[node] = node\n",
    "                    \n",
    "            nx.draw_networkx_labels(G, pos, ax=ax, labels=important_nodes, font_size=8)\n",
    "        else:\n",
    "            nx.draw_networkx_labels(G, pos, ax=ax, font_size=8)\n",
    "    \n",
    "    ax.set_title(title, fontsize=16)\n",
    "    \n",
    "    # Set the axes to reflect actual spatial coordinates\n",
    "    if node_coords:\n",
    "        x_coords = [coord[0] for coord in node_coords.values()]\n",
    "        y_coords = [coord[1] for coord in node_coords.values()]\n",
    "        ax.set_xlim(min(x_coords) - 5, max(x_coords) + 5)\n",
    "        ax.set_ylim(min(y_coords) - 5, max(y_coords) + 5)\n",
    "    \n",
    "    # Set equal aspect ratio to maintain spatial integrity\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    \n",
    "    # Turn off axis\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add colorbar for community colors if available - FIXED VERSION\n",
    "    if communities and communities['communities'] and cmap and norm:\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        sm.set_array([])\n",
    "        # Fix: Specify the axes for the colorbar\n",
    "        cbar = plt.colorbar(sm, ax=ax)\n",
    "        cbar.set_label('Community')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bef9bb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Analyze direct interactions (1-hop)\n",
    "############################################################################################\n",
    "\n",
    "def analyze_direct_and_indirect_interactions(G):\n",
    "    \"\"\"\n",
    "    Analyze direct (1-hop) and indirect (2-hop) interactions in an ecological network\n",
    "    where weights represent interaction strength (higher = stronger interaction)\n",
    "    and signs represent interaction type (positive = synergy, negative = antagonism)\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Basic network properties\n",
    "    results['num_nodes'] = G.number_of_nodes()\n",
    "    results['num_edges'] = G.number_of_edges()\n",
    "    results['density'] = nx.density(G)\n",
    "    \n",
    "    # 2. Direct interaction analysis\n",
    "    direct_interactions = analyze_direct_interactions(G)\n",
    "    results.update(direct_interactions)\n",
    "    \n",
    "    # 3. Indirect interaction analysis (2-hop paths)\n",
    "    indirect_interactions = analyze_indirect_interactions(G)\n",
    "    results.update(indirect_interactions)\n",
    "    \n",
    "    # 4. Node-level metrics\n",
    "    node_metrics = compute_node_level_metrics(G)\n",
    "    results['node_metrics'] = node_metrics\n",
    "    \n",
    "    # 5. Community detection based on interaction patterns\n",
    "    communities = detect_interaction_communities(G)\n",
    "    results['communities'] = communities\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "655bb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Summarize ecological metrics in a human-readable format\n",
    "############################################################################################\n",
    "\n",
    "def summarize_ecological_metrics(metrics):\n",
    "    \"\"\"\n",
    "    Generate a human-readable summary of the ecological metrics\n",
    "    \"\"\"\n",
    "    summary = []\n",
    "    \n",
    "    # Network overview\n",
    "    summary.append(f\"### Ecological Network Summary\")\n",
    "    summary.append(f\"Network size: {metrics['num_nodes']} species with {metrics['num_edges']} interactions\")\n",
    "    summary.append(f\"Network density: {metrics['density']:.4f}\")\n",
    "    \n",
    "    # Direct interactions\n",
    "    summary.append(f\"\\n### Direct Interactions\")\n",
    "    summary.append(f\"Positive interactions: {metrics['positive_edges']} ({metrics['avg_positive_strength']:.4f} avg strength)\")\n",
    "    summary.append(f\"Negative interactions: {metrics['negative_edges']} ({metrics['avg_negative_strength']:.4f} avg strength)\")\n",
    "    summary.append(f\"Positive-to-negative ratio: {metrics['pos_neg_ratio']:.4f}\")\n",
    "    summary.append(f\"Reciprocity: {metrics['reciprocity']:.4f}\")\n",
    "    \n",
    "    # Indirect interactions\n",
    "    if 'num_2hop_paths' in metrics:\n",
    "        summary.append(f\"\\n### Indirect (2-hop) Interactions\")\n",
    "        summary.append(f\"Total 2-hop paths: {metrics['num_2hop_paths']}\")\n",
    "        summary.append(f\"Positive indirect effects: {metrics['positive_2hop']}\")\n",
    "        summary.append(f\"Negative indirect effects: {metrics['negative_2hop']}\")\n",
    "        \n",
    "        if 'reinforcing_effects' in metrics:\n",
    "            summary.append(f\"Reinforcing effects: {metrics['reinforcing_effects']} paths\")\n",
    "            summary.append(f\"Counteracting effects: {metrics['counteracting_effects']} paths\")\n",
    "            summary.append(f\"Reinforcement ratio: {metrics['reinforcing_ratio']:.4f}\")\n",
    "    \n",
    "    # Community structure\n",
    "    if 'communities' in metrics and 'num_communities' in metrics['communities']:\n",
    "        summary.append(f\"\\n### Community Structure\")\n",
    "        summary.append(f\"Number of communities: {metrics['communities']['num_communities']}\")\n",
    "        summary.append(f\"Community detection algorithm: {metrics['communities']['algorithm']}\")\n",
    "        \n",
    "        # List community sizes\n",
    "        if metrics['communities']['communities']:\n",
    "            comm_sizes = [len(nodes) for comm_id, nodes in metrics['communities']['communities'].items()]\n",
    "            summary.append(f\"Community sizes: min={min(comm_sizes)}, max={max(comm_sizes)}, avg={np.mean(comm_sizes):.1f}\")\n",
    "    \n",
    "    # Node-level metrics\n",
    "    if 'node_metrics' in metrics:\n",
    "        # Find top species by different metrics\n",
    "        summary.append(f\"\\n### Key Species\")\n",
    "        \n",
    "        # Try to identify keystone species (high keystone score)\n",
    "        keystone_scores = {node: data.get('keystone_score', 0) \n",
    "                          for node, data in metrics['node_metrics'].items() \n",
    "                          if isinstance(data, dict) and 'keystone_score' in data}\n",
    "        \n",
    "        if keystone_scores:\n",
    "            top_keystones = sorted(keystone_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            summary.append(f\"Top keystone species (highest intermediary importance):\")\n",
    "            for i, (species, score) in enumerate(top_keystones, 1):\n",
    "                summary.append(f\"  {i}. Species {species}: score={score:.4f}\")\n",
    "        \n",
    "        # Species with highest total influence\n",
    "        closeness_scores = {node: data.get('ecological_closeness', {}).get('total_influence', 0) \n",
    "                           for node, data in metrics['node_metrics'].items() \n",
    "                           if isinstance(data, dict) and 'ecological_closeness' in data}\n",
    "        \n",
    "        if closeness_scores:\n",
    "            top_influential = sorted(closeness_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            summary.append(f\"Most influential species (highest total ecological impact):\")\n",
    "            for i, (species, score) in enumerate(top_influential, 1):\n",
    "                summary.append(f\"  {i}. Species {species}: impact={score:.4f}\")\n",
    "    \n",
    "    return \"\\n\".join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a5f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "### EXAMPLES\n",
    "############################################################################################\n",
    "\n",
    "# Example usage\n",
    "# file_path = \"weighted_matrix_combined_tick_182.csv\"\n",
    "# G = load_network_from_csv(file_path)  # Using the function from your original code\n",
    "# \n",
    "# # Analyze ecological metrics\n",
    "# metrics = analyze_direct_and_indirect_interactions(G)\n",
    "# \n",
    "# # Print summary\n",
    "# print(summarize_ecological_metrics(metrics))\n",
    "# \n",
    "# # Visualize ecological network\n",
    "# visualize_ecological_network(G, \n",
    "#                            node_metrics=metrics['node_metrics'],\n",
    "#                            communities=metrics['communities'],\n",
    "#                            title=f\"Species Interaction Network (Communities)\")\n",
    "# plt.savefig(\"ecological_network_communities.png\", dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a457e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\\weighted_matrix_with_coords_combined_tick_182.csv\"\n",
    "\n",
    "data_dir = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\"\n",
    "pattern     = os.path.join(data_dir, \"weighted_matrix_xy_combined_tick_*.csv\")\n",
    "files = glob.glob(pattern)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fb6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Load the spatial coordinates from a CSV file and create a spatial graph\n",
    "############################################################################################\n",
    "\n",
    "# Load your network\n",
    "G = load_network_from_csv(file_path)\n",
    "\n",
    "# Extract node coordinates from the CSV file\n",
    "df = pd.read_csv(file_path)\n",
    "coord_rows = pd.concat([\n",
    "    df[['source','source_x','source_y']].rename(columns={'source':'node','source_x':'x','source_y':'y'}),\n",
    "    df[['target','target_x','target_y']].rename(columns={'target':'node','target_x':'x','target_y':'y'})\n",
    "]).drop_duplicates(subset='node')\n",
    "pos = { int(r.node): (r.x, r.y) for r in coord_rows.itertuples() }\n",
    "\n",
    "# Ensure analyze_direct_interactions is defined (run CELL INDEX: 6 if not)\n",
    "metrics = analyze_direct_and_indirect_interactions(G)\n",
    "\n",
    "# Print a human-readable summary\n",
    "print(summarize_ecological_metrics(metrics))\n",
    "\n",
    "# Visualize the network with ecological interpretations\n",
    "visualize_spatial_network(G, pos, \n",
    "                         node_metrics=metrics['node_metrics'],\n",
    "                         communities=metrics['communities'],\n",
    "                         title=\"Spatial Ecological Network\",\n",
    "                         edge_width_factor=2.0,\n",
    "                         node_size_factor=50,\n",
    "                         show_labels=True)\n",
    "fig, ax = visualize_spatial_network(G, pos, \n",
    "                         node_metrics=metrics['node_metrics'],\n",
    "                         communities=metrics['communities'],\n",
    "                         title=\"Spatial Ecological Network\",\n",
    "                         edge_width_factor=2.0,\n",
    "                         node_size_factor=50,\n",
    "                         show_labels=True)\n",
    "\n",
    "# Extract tick number from file_path\n",
    "tick = int(os.path.basename(file_path).split('_')[-1].split('.')[0])\n",
    "\n",
    "ax.set_title(f\"Spatial Ecological Network at tick {tick}\")  # if you want to override\n",
    "outname = os.path.join(output_dir, f\"Spatial_Ecological_Network_{tick:03d}.png\")\n",
    "fig.savefig(outname, dpi=300, bbox_inches='tight')\n",
    "plt.close(fig)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# draw â†’ get the figure that the function creates\n",
    "fig = visualize_spatial_network(\n",
    "    G, pos,\n",
    "    node_metrics=metrics['node_metrics'],\n",
    "    communities=metrics['communities'],\n",
    "    title=f\"Network at tick {tick}\",\n",
    "    edge_width_factor=2.0,\n",
    "    node_size_factor=50,\n",
    "    show_labels=True\n",
    ")\n",
    "\n",
    "# save THAT figure\n",
    "outname = os.path.join(output_dir, f\"Spatial_Ecological_Network_{tick:03d}.png\")\n",
    "fig.savefig(outname, dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {outname} (exists: {os.path.exists(outname)})\")\n",
    "plt.show()  # show the figure\n",
    "plt.close(fig)  # free memory when looping many ticks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bf0a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SPATIAL-ECOLOGICAL INFORMATION ANALYSIS ===\n",
      "\n",
      "1. SPATIAL DISTRIBUTION PATTERNS:\n",
      "   Spatial entropy (X): 3.310 bits\n",
      "   Spatial entropy (Y): 3.318 bits\n",
      "   Max possible spatial entropy: 3.322 bits\n",
      "   Spatial uniformity: 0.998\n",
      "\n",
      "2. ECOLOGICAL INFLUENCE PATTERNS:\n",
      "   Influence entropy: -0.000 bits\n",
      "   Max influence entropy: 2.322 bits\n",
      "   Influence distribution uniformity: -0.000\n",
      "   Mean influence: 0.000\n",
      "   Std influence: 0.000\n",
      "   Influence range: [0.000, 0.000]\n",
      "\n",
      "3. COMMUNITY STRUCTURE INFORMATION:\n",
      "   Number of communities: 9\n",
      "   Community size entropy: 3.138 bits\n",
      "   Community sizes: [120, 103, 101, 80, 75, 74, 72, 69, 64]\n",
      "   Within-community edges: 6636 (0.811)\n",
      "   Between-community edges: 1544 (0.189)\n",
      "   Community connection entropy: 0.699 bits\n",
      "\n",
      "4. INTERACTION SIGN INFORMATION:\n",
      "   Positive edges: 7408 (0.906)\n",
      "   Negative edges: 772 (0.094)\n",
      "   Sign entropy: 0.451 bits\n",
      "   Sign balance: 0.811\n",
      "\n",
      "5. SPATIAL-ECOLOGICAL RELATIONSHIPS:\n",
      "   Spatial-influence correlation: nan\n",
      "\n",
      "6. HARVEST ENTROPY FOUNDATIONS:\n",
      "   Edge weight entropy: 2.262 bits\n",
      "   Mean edge weight: 0.290\n",
      "   Weight std: 0.278\n",
      "   Degree entropy: 9.521 bits\n",
      "\n",
      "=== MUTUAL INFORMATION PREPARATION ===\n",
      "Mutual Information (X-position, Influence): 0.000 bits\n",
      "Mutual Information (Y-position, Influence): 0.000 bits\n",
      "Mutual Information (X-position, Y-position): 0.001 bits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp.LAPTOP-OG98VQR6\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "c:\\Users\\hp.LAPTOP-OG98VQR6\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "# Analyze spatial information structure\n",
    "############################################################################################\n",
    "\n",
    "# After loading your network and computing metrics\n",
    "info_analysis = analyze_spatial_information_structure(\n",
    "    G, pos, metrics['node_metrics'], metrics['communities']\n",
    ")\n",
    "\n",
    "mi_analysis = prepare_mutual_information_analysis(\n",
    "    G, pos, metrics['node_metrics']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce0b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bidirectional edge pairs: 4090\n",
      "Unidirectional edges: 0\n"
     ]
    }
   ],
   "source": [
    "############################################################################################\n",
    "# Check for reciprocal edges\n",
    "############################################################################################\n",
    "\n",
    "reciprocal_edges = []\n",
    "unidirectional_edges = []\n",
    "\n",
    "for u, v in G.edges():\n",
    "    if G.has_edge(v, u):  # Check if reverse edge exists\n",
    "        reciprocal_edges.append((u, v))\n",
    "    else:\n",
    "        unidirectional_edges.append((u, v))\n",
    "\n",
    "print(f\"Bidirectional edge pairs: {len(reciprocal_edges)//2}\")  # Divide by 2 since each pair counted twice\n",
    "print(f\"Unidirectional edges: {len(unidirectional_edges)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8fafff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_network_reciprocity(G):\n",
    "\n",
    "    \"\"\"\n",
    "    Analyze the reciprocity patterns in your directed network\n",
    "    \"\"\"\n",
    "    print(\"=== NETWORK RECIPROCITY ANALYSIS ===\\n\")\n",
    "    \n",
    "    total_edges = G.number_of_edges()\n",
    "    total_possible_pairs = G.number_of_nodes() * (G.number_of_nodes() - 1) // 2\n",
    "    \n",
    "    # Count different types of relationships\n",
    "    unidirectional_pairs = 0\n",
    "    bidirectional_pairs = 0\n",
    "    isolated_pairs = 0\n",
    "    \n",
    "    # Check all possible node pairs\n",
    "    for u in G.nodes():\n",
    "        for v in G.nodes():\n",
    "            if u < v:  # Only check each pair once\n",
    "                has_uv = G.has_edge(u, v)\n",
    "                has_vu = G.has_edge(v, u)\n",
    "                \n",
    "                if has_uv and has_vu:\n",
    "                    bidirectional_pairs += 1\n",
    "                elif has_uv or has_vu:\n",
    "                    unidirectional_pairs += 1\n",
    "                else:\n",
    "                    isolated_pairs += 1\n",
    "    \n",
    "    # Calculate reciprocity metrics\n",
    "    connected_pairs = bidirectional_pairs + unidirectional_pairs\n",
    "    \n",
    "    if connected_pairs > 0:\n",
    "        reciprocity_rate = bidirectional_pairs / connected_pairs\n",
    "    else:\n",
    "        reciprocity_rate = 0\n",
    "    \n",
    "    # NetworkX built-in reciprocity (different calculation)\n",
    "    nx_reciprocity = nx.reciprocity(G)\n",
    "    \n",
    "    print(f\"Total edges: {total_edges}\")\n",
    "    print(f\"Total possible node pairs: {total_possible_pairs}\")\n",
    "    print(f\"Connected pairs: {connected_pairs}\")\n",
    "    print(f\"  - Bidirectional pairs: {bidirectional_pairs}\")\n",
    "    print(f\"  - Unidirectional pairs: {unidirectional_pairs}\")\n",
    "    print(f\"Isolated pairs: {isolated_pairs}\")\n",
    "    print(f\"\\nReciprocity rate: {reciprocity_rate:.3f}\")\n",
    "    print(f\"NetworkX reciprocity: {nx_reciprocity:.3f}\")\n",
    "    \n",
    "    # Sample some bidirectional relationships\n",
    "    print(f\"\\nSample bidirectional relationships:\")\n",
    "    bidirectional_examples = []\n",
    "    for u, v in G.edges():\n",
    "        if G.has_edge(v, u) and len(bidirectional_examples) < 5:\n",
    "            weight_uv = G[u][v]['weight']\n",
    "            weight_vu = G[v][u]['weight']\n",
    "            bidirectional_examples.append((u, v, weight_uv, weight_vu))\n",
    "    \n",
    "    for u, v, w_uv, w_vu in bidirectional_examples:\n",
    "        print(f\"  {u} â†” {v}: weights {w_uv:.3f} and {w_vu:.3f}\")\n",
    "    \n",
    "    # Sample some unidirectional relationships\n",
    "    print(f\"\\nSample unidirectional relationships:\")\n",
    "    unidirectional_examples = []\n",
    "    for u, v in G.edges():\n",
    "        if not G.has_edge(v, u) and len(unidirectional_examples) < 5:\n",
    "            weight_uv = G[u][v]['weight']\n",
    "            unidirectional_examples.append((u, v, weight_uv))\n",
    "    \n",
    "    for u, v, w_uv in unidirectional_examples:\n",
    "        print(f\"  {u} â†’ {v}: weight {w_uv:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'total_edges': total_edges,\n",
    "        'bidirectional_pairs': bidirectional_pairs,\n",
    "        'unidirectional_pairs': unidirectional_pairs,\n",
    "        'reciprocity_rate': reciprocity_rate,\n",
    "        'nx_reciprocity': nx_reciprocity\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed122eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "### Analyze the network reciprocity\n",
    "############################################################################################\n",
    "# Run this analysis on your network:\n",
    "reciprocity_stats = analyze_network_reciprocity(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4440719e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "### Prepare mutual information analysis\n",
    "############################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import networkx as nx  \n",
    "import os\n",
    "import glob\n",
    "\n",
    "def analyze_spatial_information_structure(G, node_coords, node_metrics, communities, \n",
    "                                        spatial_bins=10, influence_bins=5):\n",
    "    \"\"\"\n",
    "    Analyze the information structure of spatial ecological networks\n",
    "    for future mutual information and harvest entropy calculations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== SPATIAL-ECOLOGICAL INFORMATION ANALYSIS ===\\n\")\n",
    "    \n",
    "    # 1. SPATIAL DISTRIBUTION ANALYSIS\n",
    "    print(\"1. SPATIAL DISTRIBUTION PATTERNS:\")\n",
    "    \n",
    "    # Extract coordinates\n",
    "    coords = np.array([node_coords[node] for node in G.nodes()])\n",
    "    x_coords, y_coords = coords[:, 0], coords[:, 1]\n",
    "    \n",
    "    # Spatial entropy - how uniformly distributed are nodes in space?\n",
    "    x_bins = np.histogram(x_coords, bins=spatial_bins)[0]\n",
    "    y_bins = np.histogram(y_coords, bins=spatial_bins)[0]\n",
    "    \n",
    "    # Normalize to probabilities\n",
    "    x_probs = x_bins[x_bins > 0] / x_bins.sum()\n",
    "    y_probs = y_bins[y_bins > 0] / y_bins.sum()\n",
    "    \n",
    "    spatial_entropy_x = -np.sum(x_probs * np.log2(x_probs))\n",
    "    spatial_entropy_y = -np.sum(y_probs * np.log2(y_probs))\n",
    "    \n",
    "    print(f\"   Spatial entropy (X): {spatial_entropy_x:.3f} bits\")\n",
    "    print(f\"   Spatial entropy (Y): {spatial_entropy_y:.3f} bits\")\n",
    "    print(f\"   Max possible spatial entropy: {np.log2(spatial_bins):.3f} bits\")\n",
    "    print(f\"   Spatial uniformity: {(spatial_entropy_x + spatial_entropy_y)/(2*np.log2(spatial_bins)):.3f}\")\n",
    "    \n",
    "    # 2. ECOLOGICAL INFLUENCE DISTRIBUTION\n",
    "    print(\"\\n2. ECOLOGICAL INFLUENCE PATTERNS:\")\n",
    "    \n",
    "    influences = []\n",
    "    for node in G.nodes():\n",
    "        if node in node_metrics and 'ecological_closeness' in node_metrics[node]:\n",
    "            influence = node_metrics[node]['ecological_closeness'].get('total_influence', 0)\n",
    "            influences.append(influence)\n",
    "        else:\n",
    "            influences.append(0)\n",
    "    \n",
    "    influences = np.array(influences)\n",
    "    \n",
    "    # Influence entropy\n",
    "    influence_hist = np.histogram(influences, bins=influence_bins)[0]\n",
    "    influence_probs = influence_hist[influence_hist > 0] / influence_hist.sum()\n",
    "    influence_entropy = -np.sum(influence_probs * np.log2(influence_probs))\n",
    "    \n",
    "    print(f\"   Influence entropy: {influence_entropy:.3f} bits\")\n",
    "    print(f\"   Max influence entropy: {np.log2(influence_bins):.3f} bits\")\n",
    "    print(f\"   Influence distribution uniformity: {influence_entropy/np.log2(influence_bins):.3f}\")\n",
    "    \n",
    "    # Influence statistics\n",
    "    print(f\"   Mean influence: {np.mean(influences):.3f}\")\n",
    "    print(f\"   Std influence: {np.std(influences):.3f}\")\n",
    "    print(f\"   Influence range: [{np.min(influences):.3f}, {np.max(influences):.3f}]\")\n",
    "    \n",
    "    # 3. COMMUNITY STRUCTURE ANALYSIS\n",
    "    print(\"\\n3. COMMUNITY STRUCTURE INFORMATION:\")\n",
    "    \n",
    "    if communities and 'communities' in communities:\n",
    "        # Community size distribution\n",
    "        community_sizes = [len(nodes) for nodes in communities['communities'].values()]\n",
    "        community_size_entropy = calculate_entropy_from_counts(community_sizes)\n",
    "        \n",
    "        print(f\"   Number of communities: {len(communities['communities'])}\")\n",
    "        print(f\"   Community size entropy: {community_size_entropy:.3f} bits\")\n",
    "        print(f\"   Community sizes: {sorted(community_sizes, reverse=True)}\")\n",
    "        \n",
    "        # Modularity-like measure: within vs between community connections\n",
    "        within_community_edges = 0\n",
    "        between_community_edges = 0\n",
    "        \n",
    "        # Create node-to-community mapping\n",
    "        node_to_comm = {}\n",
    "        for comm_id, nodes in communities['communities'].items():\n",
    "            for node in nodes:\n",
    "                node_to_comm[node] = comm_id\n",
    "        \n",
    "        for u, v in G.edges():\n",
    "            if node_to_comm.get(u) == node_to_comm.get(v):\n",
    "                within_community_edges += 1\n",
    "            else:\n",
    "                between_community_edges += 1\n",
    "        \n",
    "        total_edges = within_community_edges + between_community_edges\n",
    "        if total_edges > 0:\n",
    "            within_ratio = within_community_edges / total_edges\n",
    "            between_ratio = between_community_edges / total_edges\n",
    "            \n",
    "            # Community connection entropy\n",
    "            if within_ratio > 0 and between_ratio > 0:\n",
    "                community_conn_entropy = -(within_ratio * np.log2(within_ratio) + \n",
    "                                         between_ratio * np.log2(between_ratio))\n",
    "            else:\n",
    "                community_conn_entropy = 0\n",
    "            \n",
    "            print(f\"   Within-community edges: {within_community_edges} ({within_ratio:.3f})\")\n",
    "            print(f\"   Between-community edges: {between_community_edges} ({between_ratio:.3f})\")\n",
    "            print(f\"   Community connection entropy: {community_conn_entropy:.3f} bits\")\n",
    "    \n",
    "    # 4. INTERACTION SIGN PATTERNS\n",
    "    print(\"\\n4. INTERACTION SIGN INFORMATION:\")\n",
    "    \n",
    "    positive_edges = sum(1 for _, _, d in G.edges(data=True) if d['weight'] > 0)\n",
    "    negative_edges = sum(1 for _, _, d in G.edges(data=True) if d['weight'] < 0)\n",
    "    total_edges = positive_edges + negative_edges\n",
    "    \n",
    "    if total_edges > 0:\n",
    "        pos_ratio = positive_edges / total_edges\n",
    "        neg_ratio = negative_edges / total_edges\n",
    "        \n",
    "        # Sign entropy\n",
    "        if pos_ratio > 0 and neg_ratio > 0:\n",
    "            sign_entropy = -(pos_ratio * np.log2(pos_ratio) + neg_ratio * np.log2(neg_ratio))\n",
    "        else:\n",
    "            sign_entropy = 0\n",
    "        \n",
    "        print(f\"   Positive edges: {positive_edges} ({pos_ratio:.3f})\")\n",
    "        print(f\"   Negative edges: {negative_edges} ({neg_ratio:.3f})\")\n",
    "        print(f\"   Sign entropy: {sign_entropy:.3f} bits\")\n",
    "        print(f\"   Sign balance: {pos_ratio - neg_ratio:.3f}\")\n",
    "    \n",
    "    # 5. SPATIAL-ECOLOGICAL CORRELATION PREPARATION\n",
    "    print(\"\\n5. SPATIAL-ECOLOGICAL RELATIONSHIPS:\")\n",
    "    \n",
    "    # Prepare data for future mutual information analysis\n",
    "    spatial_data = {\n",
    "        'x_coords': x_coords,\n",
    "        'y_coords': y_coords,\n",
    "        'influences': influences,\n",
    "        'distances_to_center': np.sqrt((x_coords - np.mean(x_coords))**2 + \n",
    "                                     (y_coords - np.mean(y_coords))**2)\n",
    "    }\n",
    "    \n",
    "    # Spatial autocorrelation of influences\n",
    "    # Calculate pairwise distances\n",
    "    distances = squareform(pdist(coords))\n",
    "    \n",
    "    # Calculate influence correlation vs distance\n",
    "    influence_pairs = []\n",
    "    distance_pairs = []\n",
    "    \n",
    "    for i in range(len(influences)):\n",
    "        for j in range(i+1, len(influences)):\n",
    "            influence_pairs.append(influences[i] * influences[j])\n",
    "            distance_pairs.append(distances[i, j])\n",
    "    \n",
    "    # Correlation between influence product and spatial distance\n",
    "    if len(influence_pairs) > 1:\n",
    "        spatial_influence_corr = np.corrcoef(influence_pairs, distance_pairs)[0, 1]\n",
    "        print(f\"   Spatial-influence correlation: {spatial_influence_corr:.3f}\")\n",
    "    \n",
    "    # 6. HARVEST ENTROPY PREPARATION\n",
    "    print(\"\\n6. HARVEST ENTROPY FOUNDATIONS:\")\n",
    "    \n",
    "    # Edge weight distribution (for future harvest calculations)\n",
    "    weights = [abs(d['weight']) for _, _, d in G.edges(data=True)]\n",
    "    if weights:\n",
    "        weight_entropy = calculate_entropy_from_values(weights, bins=10)\n",
    "        print(f\"   Edge weight entropy: {weight_entropy:.3f} bits\")\n",
    "        print(f\"   Mean edge weight: {np.mean(weights):.3f}\")\n",
    "        print(f\"   Weight std: {np.std(weights):.3f}\")\n",
    "    \n",
    "    # Node degree distribution (connectivity entropy)\n",
    "    degrees = [G.degree(node) for node in G.nodes()]\n",
    "    degree_entropy = calculate_entropy_from_counts(degrees)\n",
    "    print(f\"   Degree entropy: {degree_entropy:.3f} bits\")\n",
    "    \n",
    "    return {\n",
    "        'spatial_entropy': (spatial_entropy_x + spatial_entropy_y) / 2,\n",
    "        'influence_entropy': influence_entropy,\n",
    "        'community_structure': communities,\n",
    "        'sign_entropy': sign_entropy if 'sign_entropy' in locals() else 0,\n",
    "        'spatial_data': spatial_data,\n",
    "        'network_stats': {\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'density': G.number_of_edges() / (G.number_of_nodes() * (G.number_of_nodes() - 1))\n",
    "        }\n",
    "    }\n",
    "\n",
    "def calculate_entropy_from_counts(counts):\n",
    "    \"\"\"Calculate entropy from a list of counts\"\"\"\n",
    "    counts = np.array(counts)\n",
    "    counts = counts[counts > 0]  # Remove zeros\n",
    "    probs = counts / counts.sum()\n",
    "    return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "def calculate_entropy_from_values(values, bins=10):\n",
    "    \"\"\"Calculate entropy from continuous values by binning\"\"\"\n",
    "    hist, _ = np.histogram(values, bins=bins)\n",
    "    return calculate_entropy_from_counts(hist)\n",
    "\n",
    "def prepare_mutual_information_analysis(G, node_coords, node_metrics, spatial_bins=5):\n",
    "    \"\"\"\n",
    "    Prepare discretized data for mutual information calculations\n",
    "    \"\"\"\n",
    "    print(\"\\n=== MUTUAL INFORMATION PREPARATION ===\")\n",
    "    \n",
    "    # Discretize spatial coordinates\n",
    "    coords = np.array([node_coords[node] for node in G.nodes()])\n",
    "    x_coords, y_coords = coords[:, 0], coords[:, 1]\n",
    "    \n",
    "    # Create spatial bins\n",
    "    x_digitized = np.digitize(x_coords, np.linspace(x_coords.min(), x_coords.max(), spatial_bins))\n",
    "    y_digitized = np.digitize(y_coords, np.linspace(y_coords.min(), y_coords.max(), spatial_bins))\n",
    "    \n",
    "    # Discretize influences\n",
    "    influences = []\n",
    "    for node in G.nodes():\n",
    "        if node in node_metrics and 'ecological_closeness' in node_metrics[node]:\n",
    "            influence = node_metrics[node]['ecological_closeness'].get('total_influence', 0)\n",
    "            influences.append(influence)\n",
    "        else:\n",
    "            influences.append(0)\n",
    "    \n",
    "    influences = np.array(influences)\n",
    "    influence_digitized = np.digitize(influences, np.linspace(influences.min(), influences.max(), spatial_bins))\n",
    "    \n",
    "    # Calculate mutual information between spatial and ecological variables\n",
    "    mi_x_influence = mutual_info_score(x_digitized, influence_digitized)\n",
    "    mi_y_influence = mutual_info_score(y_digitized, influence_digitized)\n",
    "    mi_xy_spatial = mutual_info_score(x_digitized, y_digitized)\n",
    "    \n",
    "    print(f\"Mutual Information (X-position, Influence): {mi_x_influence:.3f} bits\")\n",
    "    print(f\"Mutual Information (Y-position, Influence): {mi_y_influence:.3f} bits\")\n",
    "    print(f\"Mutual Information (X-position, Y-position): {mi_xy_spatial:.3f} bits\")\n",
    "    \n",
    "    return {\n",
    "        'spatial_discretized': (x_digitized, y_digitized),\n",
    "        'influence_discretized': influence_digitized,\n",
    "        'mutual_info': {\n",
    "            'x_influence': mi_x_influence,\n",
    "            'y_influence': mi_y_influence,\n",
    "            'xy_spatial': mi_xy_spatial\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d78a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "###DEBUGGED with ChatGPT\n",
    "###ChatGPT-04-mini-high###\n",
    "############################################\n",
    "### XY Netorks with Entropy calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "### XY Netorks with Entropy calculations\n",
    "############################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import networkx as nx  \n",
    "import os\n",
    "import glob\n",
    "\n",
    "data_dir = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\"\n",
    "# data_dir = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation3\"\n",
    "# data_dir = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation4\n",
    "# data_dir = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation5\"\n",
    "pattern = os.path.join(data_dir, \"weighted_matrix_with_coords_combined_tick_*.csv\")\n",
    "files = glob.glob(pattern)\n",
    "# sort by tick-number at end of filename\n",
    "files = sorted(files, key=lambda fp: int(os.path.basename(fp).split('_')[-1].split('.')[0]))\n",
    "\n",
    "ticks = []\n",
    "entropies = []\n",
    "\n",
    "for fp in files:\n",
    "    # 1) extract tick\n",
    "    tick = int(os.path.basename(fp).split('_')[-1].split('.')[0])\n",
    "    ticks.append(tick)\n",
    "\n",
    "    # 2) read csv\n",
    "    df = pd.read_csv(fp)\n",
    "    \n",
    "    # 3) compute entropy\n",
    "    # pivot to adjacency\n",
    "    adj = df.pivot(index='source', columns='target', values='weight').fillna(0)\n",
    "    P   = adj.div(adj.sum(axis=1), axis=0)\n",
    "    p   = P.values.flatten()\n",
    "    p   = p[p>0]\n",
    "    p  /= p.sum()\n",
    "    H   = -(p * np.log2(p)).sum()\n",
    "    entropies.append(H)\n",
    "    print(f\"tick {tick:3d}: H = {H:.4f} bits\")\n",
    "\n",
    "    # 4) build graph\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in df.iterrows():\n",
    "        u, v, w = int(row.source), int(row.target), float(row.weight)\n",
    "        G.add_edge(u, v, weight=w)\n",
    "    # 5) build pos dict from your source_x/source_y and target_x/target_y\n",
    "    coord_rows = pd.concat([\n",
    "        df[['source','source_x','source_y']].rename(columns={'source':'node','source_x':'x','source_y':'y'}),\n",
    "        df[['target','target_x','target_y']].rename(columns={'target':'node','target_x':'x','target_y':'y'})\n",
    "    ]).drop_duplicates(subset='node')\n",
    "    pos = { int(r.node): (r.x, r.y) for r in coord_rows.itertuples() }\n",
    "\n",
    "    # 6) visualize at true coordinates\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    # edge widths/colors\n",
    "    edges = G.edges(data=True)\n",
    "    weights = [abs(d['weight'])*3 for _,_,d in edges]\n",
    "    colors  = [      d['weight']   for _,_,d in edges]\n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=30, node_color='k', alpha=0.7)\n",
    "    # edges\n",
    "    ec = nx.draw_networkx_edges(\n",
    "        G, pos, ax=ax,\n",
    "        arrowstyle='-',\n",
    "        width=weights,\n",
    "        edge_color=colors,\n",
    "        edge_cmap=plt.cm.RdBu_r,\n",
    "        edge_vmin=-max(abs(min(colors)),abs(max(colors))),\n",
    "        edge_vmax= max(abs(min(colors)),abs(max(colors))),\n",
    "        alpha=0.6\n",
    "    )\n",
    "    # colorbar\n",
    "    sm = plt.cm.ScalarMappable(\n",
    "        cmap=plt.cm.RdBu_r,\n",
    "        norm=plt.Normalize(\n",
    "            vmin=-max(abs(min(colors)),abs(max(colors))),\n",
    "             vmax= max(abs(min(colors)),abs(max(colors)))\n",
    "        )\n",
    "    )\n",
    "    sm.set_array([])\n",
    "    fig.colorbar(sm, ax=ax, label='edge weight')\n",
    "    ax.set_title(f\"Tick {tick}  â€”  H={H:.3f} bits\")\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"network_tick_{tick:03d}.png\", dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "# 7) finally plot entropy series\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(ticks, entropies, '-o')\n",
    "plt.xlabel('Tick')\n",
    "plt.ylabel('Shannon entropy (bits)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b000f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "### XY Netorks with Entropy calculations Edit - 1\n",
    "############################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import networkx as nx  \n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "# 1) find & sort your files\n",
    "#data_dir    = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation5\"\n",
    "data_dir   = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\"\n",
    "\n",
    "pattern     = os.path.join(data_dir, \"weighted_matrix_with_coords_combined_tick_*.csv\")\n",
    "files       = glob.glob(pattern)\n",
    "files.sort(key=lambda fp: int(os.path.basename(fp).split('_')[-1].split('.')[0]))\n",
    "\n",
    "# helper to pull tick number out of filename\n",
    "def extract_tick(fp):\n",
    "    return int(os.path.basename(fp).split('_')[-1].split('.')[0])\n",
    "\n",
    "# the core loop\n",
    "ticks      = []\n",
    "entropies  = []\n",
    "\n",
    "for fp in files:\n",
    "    tick = extract_tick(fp)\n",
    "    ticks.append(tick)\n",
    "\n",
    "    # --- read & entropy ---\n",
    "    df = pd.read_csv(fp)\n",
    "    adj = df.pivot(index='source', columns='target', values='weight').fillna(0)\n",
    "    P   = adj.div(adj.sum(axis=1), axis=0)\n",
    "    p   = P.values.flatten()\n",
    "    p   = p[p > 0]\n",
    "    p  /= p.sum()\n",
    "    H   = -(p * np.log2(p)).sum()\n",
    "    entropies.append(H)\n",
    "    print(f\"tick {tick:3d}: H = {H:.4f} bits\")\n",
    "\n",
    "    # --- build graph & extract real positions ---\n",
    "    G = nx.DiGraph()\n",
    "    pos = {}\n",
    "    for _, row in df.iterrows():\n",
    "        src, tgt = int(row.source), int(row.target)\n",
    "        w        = row.weight\n",
    "\n",
    "        # add nodes + record their true x/y\n",
    "        if src not in G:\n",
    "            G.add_node(src)\n",
    "            pos[src] = (row.source_x, row.source_y)\n",
    "        if tgt not in G:\n",
    "            G.add_node(tgt)\n",
    "            pos[tgt] = (row.target_x, row.target_y)\n",
    "\n",
    "        # add the directed edge\n",
    "        G.add_edge(src, tgt, weight=w)\n",
    "\n",
    "    # --- visualize with reversed cmap ---\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos, ax=ax,\n",
    "        node_size=50,\n",
    "        node_color='green',\n",
    "        alpha=0.8\n",
    "    )\n",
    "\n",
    "    # edge styling\n",
    "    weights = [abs(d['weight']) * 2 for _,_,d in G.edges(data=True)]\n",
    "    colors  = [d['weight']        for _,_,d in G.edges(data=True)]\n",
    "    M       = max(abs(min(colors)), abs(max(colors)))\n",
    "\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos, ax=ax,\n",
    "        width=weights,\n",
    "        edge_color=colors,\n",
    "        edge_cmap=plt.cm.RdBu_r,    # reversed RdBu\n",
    "        edge_vmin=-M,\n",
    "        edge_vmax=+M,\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "    sm = plt.cm.ScalarMappable(\n",
    "        cmap=plt.cm.RdBu_r,\n",
    "        norm=plt.Normalize(vmin=-M, vmax=+M)\n",
    "    )\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, ax=ax)\n",
    "    cbar.set_label(\"Edge weight\")\n",
    "\n",
    "    ax.set_title(f\"Network at tick {tick}\")\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    outname = f\"network_tick_{tick:03d}.png\"\n",
    "    plt.savefig(outname, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "# --- final H vs tick plot ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ticks, entropies, '-o')\n",
    "plt.xlabel(\"Tick\")\n",
    "plt.ylabel(\"Shannon entropy (bits)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"entropy_vs_tick.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d723a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "### XY Netorks with Entropy calculations Edit - 2\n",
    "############################################################################################\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import networkx as nx  \n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "data_dir    = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\"\n",
    "# data_dir    = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation5\"\n",
    "\n",
    "pattern = os.path.join(data_dir, \"weighted_matrix_with_coords_combined_tick_*.csv\")\n",
    "files = glob.glob(pattern)\n",
    "# sort by tick-number at end of filename\n",
    "files = sorted(files, key=lambda fp: int(os.path.basename(fp).split('_')[-1].split('.')[0]))\n",
    "\n",
    "# Define the output directory for network plots\n",
    "output_dir = os.path.join(data_dir, \"network_plots\\diagnostic_plots\")\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "# the core loop\n",
    "ticks      = []\n",
    "entropies  = []\n",
    "\n",
    "for fp in files:\n",
    "    # 1) extract tick\n",
    "    tick = int(os.path.basename(fp).split('_')[-1].split('.')[0])\n",
    "    ticks.append(tick)\n",
    "\n",
    "    # 2) read csv\n",
    "    df = pd.read_csv(fp)\n",
    "    \n",
    "    # 3) compute entropy\n",
    "    # pivot to adjacency\n",
    "    adj = df.pivot(index='source', columns='target', values='weight').fillna(0)\n",
    "    P   = adj.div(adj.sum(axis=1), axis=0)\n",
    "    p   = P.values.flatten()\n",
    "    p   = p[p>0]\n",
    "    p  /= p.sum()\n",
    "    H   = -(p * np.log2(p)).sum()\n",
    "    entropies.append(H)\n",
    "    print(f\"tick {tick:3d}: H = {H:.4f} bits\")\n",
    "\n",
    "    # 4) build graph\n",
    "    G = nx.DiGraph()\n",
    "    for _, row in df.iterrows():\n",
    "        u, v, w = int(row.source), int(row.target), float(row.weight)\n",
    "        G.add_edge(u, v, weight=w)\n",
    "    # 5) build pos dict from your source_x/source_y and target_x/target_y\n",
    "    coord_rows = pd.concat([\n",
    "        df[['source','source_x','source_y']].rename(columns={'source':'node','source_x':'x','source_y':'y'}),\n",
    "        df[['target','target_x','target_y']].rename(columns={'target':'node','target_x':'x','target_y':'y'})\n",
    "    ]).drop_duplicates(subset='node')\n",
    "    pos = { int(r.node): (r.x, r.y) for r in coord_rows.itertuples() }\n",
    "\n",
    "    # 6) visualize at true coordinates\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    # edge widths/colors\n",
    "    # 6) visualize at true coordinates\n",
    "    edges = G.edges(data=True)\n",
    "    weights = [abs(d['weight'])*3 for _,_,d in edges]\n",
    "    colors  = [      d['weight']   for _,_,d in edges]\n",
    "\n",
    "    # nodes - changed to green\n",
    "    nx.draw_networkx_nodes(G, pos, ax=ax, node_size=30, node_color='green', alpha=0.7)\n",
    "\n",
    "    # edges - inverted colormap: negative edges = red, positive edges = blue\n",
    "    ec = nx.draw_networkx_edges(\n",
    "        G, pos, ax=ax,\n",
    "        arrowstyle='-',\n",
    "        width=weights,\n",
    "        edge_color=colors,\n",
    "        edge_cmap=plt.cm.RdBu,    # _r reverses the colormap: red for negative, blue for positive\n",
    "        edge_vmin=-max(abs(min(colors)),abs(max(colors))),\n",
    "        edge_vmax= max(abs(min(colors)),abs(max(colors))),\n",
    "        alpha=0.6\n",
    "    )\n",
    "\n",
    "    # colorbar - also use reversed colormap\n",
    "    sm = plt.cm.ScalarMappable(\n",
    "        cmap=plt.cm.RdBu,  # _r to match the edges\n",
    "        norm=plt.Normalize(\n",
    "            vmin=-max(abs(min(colors)),abs(max(colors))),\n",
    "            vmax= max(abs(min(colors)),abs(max(colors)))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    sm.set_array([])\n",
    "    fig.colorbar(sm, ax=ax, label='edge weight')\n",
    "    ax.set_title(f\"Tick {tick}  â€”  H={H:.3f} bits\")\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    outpath = os.path.join(output_dir, f\"network_tick_{tick:03d}.png\")\n",
    "    plt.savefig(outpath, dpi=300)\n",
    "    print(f\"  â†’ saved {outpath}\")\n",
    "    plt.close(fig)\n",
    "# --- final H vs tick plot ---\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(ticks, entropies, '-o')\n",
    "plt.xlabel(\"Tick\")\n",
    "plt.ylabel(\"Shannon entropy (bits)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, \"entropy_vs_tick.png\"), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31018810",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "### XY Netorks ECOLOGICAL METRICS & VISUALIZATION - FINAL\n",
    "#############################################################################################\n",
    "\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- paths ---\n",
    "data_dir = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\"\n",
    "pattern   = os.path.join(data_dir, \"weighted_matrix_with_coords_combined_tick_*.csv\")\n",
    "\n",
    "# collect & sort by tick number\n",
    "files = sorted(\n",
    "    glob.glob(pattern),\n",
    "    key=lambda fp: int(os.path.basename(fp).split('_')[-1].split('.')[0])\n",
    ")\n",
    "\n",
    "# output dir\n",
    "output_dir = os.path.join(data_dir, \"network_graphs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def extract_tick(fp):\n",
    "    return int(os.path.basename(fp).split('_')[-1].split('.')[0])\n",
    "\n",
    "ticks = []\n",
    "\n",
    "for fp in files:\n",
    "    try:\n",
    "        # 1) tick\n",
    "        tick = extract_tick(fp)\n",
    "        ticks.append(tick)\n",
    "\n",
    "        # 2) read this file's CSV ONCE\n",
    "        df = pd.read_csv(fp)\n",
    "\n",
    "        # 3) build graph for THIS file (important: use fp, not file_path)\n",
    "        G = load_network_from_csv(fp)\n",
    "\n",
    "        # 4) node coordinates for THIS file\n",
    "        coord_rows = pd.concat([\n",
    "            df[['source','source_x','source_y']].rename(\n",
    "                columns={'source':'node','source_x':'x','source_y':'y'}\n",
    "            ),\n",
    "            df[['target','target_x','target_y']].rename(\n",
    "                columns={'target':'node','target_x':'x','target_y':'y'}\n",
    "            ),\n",
    "        ], ignore_index=True).drop_duplicates(subset='node')\n",
    "\n",
    "        # ensure node ids line up with G; if your nodes are ints, cast:\n",
    "        try:\n",
    "            coord_rows['node'] = coord_rows['node'].astype(int)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        pos = {row['node']: (row['x'], row['y']) for _, row in coord_rows.iterrows()}\n",
    "\n",
    "        # 5) metrics\n",
    "        metrics = analyze_direct_and_indirect_interactions(G)\n",
    "        print(summarize_ecological_metrics(metrics))\n",
    "\n",
    "        # 6) draw & save\n",
    "        fig, ax = visualize_spatial_network(\n",
    "        G, pos,\n",
    "        node_metrics=metrics.get('node_metrics', {}),\n",
    "        communities=metrics.get('communities', {}),\n",
    "        title=f\"Spatial Ecological Network â€“ tick {tick}\",\n",
    "        edge_width_factor=2.0,\n",
    "        node_size_factor=50,\n",
    "        show_labels=True\n",
    "    )\n",
    "\n",
    "        outname = os.path.join(output_dir, f\"Spatial_Ecological_Network_{tick:03d}.png\")\n",
    "        fig.savefig(outname, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)  # important to avoid memory bloat in loops\n",
    "        print(f\"  Saved: {outname}\")\n",
    "\n",
    "        # (optional) briefly show\n",
    "        from IPython.display import display\n",
    "        display(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Skipping tick {tick} due to error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286528bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "### XY Netorks ECOLOGICAL METRICS & VISUALIZATION - FINAL 2\n",
    "#############################################################################################\n",
    "\n",
    "import os, glob, sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- config ----------\n",
    "data_dir   = r\"C:\\Users\\hp.LAPTOP-OG98VQR6\\OneDrive\\Desktop\\ERSA\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\"\n",
    "pattern    = os.path.join(data_dir, \"weighted_matrix_with_coords_combined_tick_*.csv\")\n",
    "output_dir = os.path.join(data_dir, \"network_graphs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def extract_tick(fp):\n",
    "    try:\n",
    "        return int(os.path.basename(fp).split('_')[-1].split('.')[0])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---------- discover files ----------\n",
    "files = glob.glob(pattern)\n",
    "print(f\"[INFO] Searching: {pattern}\")\n",
    "print(f\"[INFO] Found {len(files)} files\")\n",
    "\n",
    "# Show a few examples so we can see the naming\n",
    "for f in sorted(files)[:5]:\n",
    "    print(\"  ->\", os.path.basename(f))\n",
    "\n",
    "# If nothing found, stop here\n",
    "if not files:\n",
    "    print(\"[ERROR] No files matched the pattern. Check data_dir/pattern.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Sort by tick; skip any file we can't parse\n",
    "files = sorted(\n",
    "    (f for f in files if extract_tick(f) is not None),\n",
    "    key=lambda fp: extract_tick(fp)\n",
    ")\n",
    "print(f\"[INFO] After sorting by tick: {len(files)} files\")\n",
    "\n",
    "# ---------- main loop ----------\n",
    "for fp in files:\n",
    "    tick = extract_tick(fp)\n",
    "    print(f\"\\n[INFO] Processing tick {tick}: {os.path.basename(fp)}\")\n",
    "\n",
    "    # 1) Read once and check columns\n",
    "    df = pd.read_csv(fp)\n",
    "    print(f\"[INFO] CSV rows: {len(df)} | columns: {list(df.columns)}\")\n",
    "\n",
    "    needed = ['source','target','weight','source_x','source_y','target_x','target_y']\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        print(f\"[ERROR] Missing columns in {os.path.basename(fp)}: {missing}\")\n",
    "        continue\n",
    "\n",
    "    # 2) Build graph for THIS file\n",
    "    G = load_network_from_csv(fp)  # this should print its own summary; if it doesn't, we didn't call it\n",
    "    # 3) Position dict\n",
    "    coords = pd.concat([\n",
    "        df[['source','source_x','source_y']].rename(columns={'source':'node','source_x':'x','source_y':'y'}),\n",
    "        df[['target','target_x','target_y']].rename(columns={'target':'node','target_x':'x','target_y':'y'})\n",
    "    ], ignore_index=True).drop_duplicates(subset='node')\n",
    "\n",
    "    try:\n",
    "        coords['node'] = coords['node'].astype(int)\n",
    "    except Exception:\n",
    "        pass\n",
    "    pos = {int(r.node): (float(r.x), float(r.y)) for _, r in coords.iterrows()}\n",
    "\n",
    "    # 4) Metrics\n",
    "    metrics = analyze_direct_and_indirect_interactions(G)\n",
    "    print(summarize_ecological_metrics(metrics))\n",
    "\n",
    "    # 5) Draw\n",
    "    fig, ax = visualize_spatial_network(\n",
    "    G, pos,\n",
    "    node_metrics=metrics.get('node_metrics', {}),\n",
    "    communities=metrics.get('communities', {}),\n",
    "    title=f\"Spatial Ecological Network â€“ tick {tick}\",\n",
    "    edge_width_factor=2.0,\n",
    "    node_size_factor=50,\n",
    "    show_labels=True\n",
    ")\n",
    "\n",
    "    if fig is None:\n",
    "        print(\"[ERROR] visualize_spatial_network returned None. \"\n",
    "              \"Ensure it RETURNS the matplotlib Figure and does NOT call plt.close() inside.\")\n",
    "        continue\n",
    "\n",
    "    # 6) Save THEN (optionally) show\n",
    "    outname = os.path.join(output_dir, f\"Spatial_Ecological_Network_{tick:03d}.png\")\n",
    "    fig.savefig(outname, dpi=300, bbox_inches='tight')\n",
    "    print(f\"[OK] Saved: {outname}\")\n",
    "\n",
    "    outname = os.path.join(output_dir, f\"Spatial_Ecological_Network_{tick:03d}.png\")\n",
    "    fig.savefig(outname, dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    print(f\"[OK] Saved: {outname}\")\n",
    "\n",
    "    # If you want to see the figure interactively, show BEFORE closing:\n",
    "    # plt.show(block=False); plt.pause(0.5)\n",
    "\n",
    "    plt.close(fig)  # free memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5427121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of the requested columns are present in the dataframe. Available columns are:\n",
      "['source', 'target', 'weight', 'type', 'link-interaction-type', 'source_x', 'source_y', 'target_x', 'target_y']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, os\n",
    "# If df_ts is already in memory, use it; otherwise read from disk\n",
    "try:\n",
    "    df = df.copy()\n",
    "except NameError:\n",
    "    df = pd.read_csv(os.path.join(output_dir, 'all_metrics.csv'), index_col=0)\n",
    "\n",
    "# Check which columns are actually present in the dataframe\n",
    "available_cols = [col for col in [\n",
    "    'degree_entropy','weighted_degree_entropy','community_entropy','num_communities','modularity',\n",
    "    'kl_weight_vs_harvest','js_weight_vs_harvest','harvest_entropy','pearson_w_h',\n",
    "    'js_boot_mean','js_boot_lo','js_boot_hi'\n",
    "] if col in df.columns]\n",
    "\n",
    "if not available_cols:\n",
    "    print(\"None of the requested columns are present in the dataframe. Available columns are:\")\n",
    "    print(df.columns.tolist())\n",
    "else:\n",
    "    sel = df.loc[[52,182,208,234,260,390], available_cols]\n",
    "    print(sel.round(9))\n",
    "    # Optional: export a small CSV for the manuscript folder\n",
    "    sel.round(9).to_csv(os.path.join(output_dir, 'selected_metrics_ticks_52_182_208_234_260_390.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
