{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feecb620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "### This script computes various network metrics from CSV files containing\n",
    "### weighted adjacency matrices with coordinates.\n",
    "### DEBUGGED AND EDITED\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4be3659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from  scipy.special import rel_entr\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import community\n",
    "import seaborn as sns\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46604a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "# Define the directory and file patterns\n",
    "# Adjust the paths as necessary for your environment\n",
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb41e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "### Simulations with High Initial Soil Fertility (Index 8400) and 40% Initial Reposition (adjustable)\n",
    "#############################################################################################\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "data_dir = r\"C:\\Users\\myPC\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\"\n",
    "file_path = r\"C:\\Users\\myPC\\Agroforestal\\Discover Networks\\Simulations\\Simulation6\\weighted_matrix_with_coords_combined_tick_182.csv\"\n",
    "\n",
    "# Collect all files matching the pattern\n",
    "\n",
    "pattern      = os.path.join(data_dir, \"weighted_matrix_with_coords_combined_tick_*.csv\")\n",
    "harvest_file = os.path.join(data_dir, \"harvest_per_plant_40RepositionVariable.csv\")\n",
    "harvest_df   = pd.read_csv(harvest_file)\n",
    "\n",
    "# Clean up: convert 'tick' and 'harvested' to numeric, drop rows with NaN\n",
    "harvest_df['tick'] = pd.to_numeric(harvest_df['tick'], errors='coerce')\n",
    "harvest_df['harvested'] = pd.to_numeric(harvest_df['harvested'], errors='coerce')\n",
    "harvest_df = harvest_df.dropna(subset=['tick', 'harvested'])\n",
    "\n",
    "# make a dict:  tick → np.array of harvested values (length 400 each tick)\n",
    "harvest_by_tick = {\n",
    "    int(t): grp[\"harvested\"].to_numpy(float)\n",
    "    for t, grp in harvest_df.groupby(\"tick\")\n",
    "}\n",
    "\n",
    "output_dir = os.path.join(data_dir, \"network_plots\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "save_dir     = output_dir\n",
    "\n",
    "files = sorted(glob.glob(pattern),          # FIX glob call\n",
    "               key=lambda fp: int(os.path.basename(fp).split('_')[-1].split('.')[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cfba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "### Simulations with High Initial Soil Fertility (Index 8400) and 45% Initial Reposition (fixed)\n",
    "#############################################################################################\n",
    "data_dir = r\"C:\\Users\\myPC\\Agroforestal\\Discover Networks\\Simulations\\Simulation5\"\n",
    "file_path = r\"C:\\Users\\myPC\\Agroforestal\\Discover Networks\\Simulations\\Simulation5\\weighted_matrix_with_coords_combined_tick_182.csv\"\n",
    "\n",
    "# Collect all files matching the pattern\n",
    "\n",
    "pattern      = os.path.join(data_dir, \"weighted_matrix_with_coords_combined_tick_*.csv\")\n",
    "harvest_file = os.path.join(data_dir, \"harvest_per_plant_45FixedReposition.csv\")\n",
    "harvest_df   = pd.read_csv(harvest_file)\n",
    "\n",
    "# Clean up: convert 'tick' and 'harvested' to numeric, drop rows with NaN\n",
    "harvest_df['tick'] = pd.to_numeric(harvest_df['tick'], errors='coerce')\n",
    "harvest_df['harvested'] = pd.to_numeric(harvest_df['harvested'], errors='coerce')\n",
    "harvest_df = harvest_df.dropna(subset=['tick', 'harvested'])\n",
    "\n",
    "# make a dict:  tick → np.array of harvested values (length 400 each tick)\n",
    "harvest_by_tick = {\n",
    "    int(t): grp[\"harvested\"].to_numpy(float)\n",
    "    for t, grp in harvest_df.groupby(\"tick\")\n",
    "}\n",
    "\n",
    "output_dir = os.path.join(data_dir, \"network_plots\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "files = sorted(glob.glob(pattern),          # FIX glob call\n",
    "               key=lambda fp: int(os.path.basename(fp).split('_')[-1].split('.')[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039e2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "### Simulations with Low Initial Soil Fertility (Index 4400) and 50% Initial Reposition (adjustable)\n",
    "#############################################################################################\n",
    "data_dir = r\"C:\\Users\\myPC\\Agroforestal\\Discover Networks\\Simulations\\Simulation7\"\n",
    "file_path = r\"C:\\Users\\myPC\\Agroforestal\\Discover Networks\\Simulations\\Simulation7\\weighted_matrix_with_coords_combined_tick_182.csv\"\n",
    "\n",
    "# Collect all files matching the pattern\n",
    "\n",
    "pattern      = os.path.join(data_dir, \"weighted_matrix_with_coords_combined_tick_*.csv\")\n",
    "harvest_file = os.path.join(data_dir, \"harvest_per_plant_50RepositionVariable.csv\")\n",
    "harvest_df   = pd.read_csv(harvest_file)\n",
    "\n",
    "# Clean up: convert 'tick' and 'harvested' to numeric, drop rows with NaN\n",
    "harvest_df['tick'] = pd.to_numeric(harvest_df['tick'], errors='coerce')\n",
    "harvest_df['harvested'] = pd.to_numeric(harvest_df['harvested'], errors='coerce')\n",
    "harvest_df = harvest_df.dropna(subset=['tick', 'harvested'])\n",
    "\n",
    "# make a dict:  tick → np.array of harvested values (length 400 each tick)\n",
    "harvest_by_tick = {\n",
    "    int(t): grp[\"harvested\"].to_numpy(float)\n",
    "    for t, grp in harvest_df.groupby(\"tick\")\n",
    "}\n",
    "\n",
    "output_dir = os.path.join(data_dir, \"network_plots\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "files = sorted(glob.glob(pattern),          # FIX glob call\n",
    "               key=lambda fp: int(os.path.basename(fp).split('_')[-1].split('.')[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "# Define the target ticks for analysis and define dictionaries for network, harvest, and soil files\n",
    "#############################################################################################\n",
    "\n",
    "target_ticks = [128, 180, 336, 388, 440, 492]\n",
    "# target_ticks = [24, 76, 128, 180, 232, 284, 336, 388, 440, 492]\n",
    "\n",
    "for tick in target_ticks:\n",
    "    print(f\"Preparing files for tick {tick}\")\n",
    "    \n",
    "    # Create dictionaries for each tick\n",
    "    network_files = {tick: f\"{data_dir}/weighted_matrix_with_coords_combined_tick_{tick + 2}.csv\"}\n",
    "    harvest_file = os.path.join(data_dir, \"harvest_per_plant_40RepositionVariable.csv\")\n",
    "    soil_file = f\"{data_dir}/comprehensive_soil_data_40Variable.csv\"\n",
    "\n",
    "    weights_file = fr\"{data_dir}\\weighted_matrix_xy_combined_tick_{tick+ 2}.csv\"\n",
    "    W = pd.read_csv(weights_file, index_col=0).to_numpy(float)\n",
    "\n",
    "    # flatten all finite weights, keep negatives too\n",
    "    weights_flat = W[np.isfinite(W)].ravel()\n",
    "    harvest_vec = harvest_by_tick[tick-2]       # length 400\n",
    "\n",
    "    \n",
    "    # → do whatever with 'metrics'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c817e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Load network from CSV file - Version 2 for Pearson weight–harvest correlations\n",
    "############################################################################################\n",
    "\n",
    "import networkx as nx\n",
    "import re, pandas as pd\n",
    "\n",
    "spec_re = re.compile(r\"species(\\d+)-species(\\d+)\")\n",
    "\n",
    "def load_network_from_csv(fp: str) -> nx.Graph:\n",
    "    df = pd.read_csv(fp)                      # the edge list file\n",
    "    G  = nx.DiGraph()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        u, v, w = int(row['source']), int(row['target']), row['weight']\n",
    "        G.add_edge(u, v, weight=w)\n",
    "\n",
    "        # --- add / check species attribute ----------------------\n",
    "        m = spec_re.fullmatch(row['type'])\n",
    "        if m:\n",
    "            su, sv = int(m.group(1)), int(m.group(2))\n",
    "            if 'species' not in G.nodes[u]:\n",
    "                G.nodes[u]['species'] = su\n",
    "            if 'species' not in G.nodes[v]:\n",
    "                G.nodes[v]['species'] = sv\n",
    "        else:\n",
    "            raise ValueError(f\"type column not parsable: {row['type']}\")\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Load network from CSV file - Version 1\n",
    "############################################################################################\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "def load_network_from_csv(file_path):\n",
    "    \"\"\"\n",
    "    Load network data from CSV file into a NetworkX graph\n",
    "    \"\"\"\n",
    "    print(f\"Loading network from: {file_path}\")\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loaded data with {len(df)} rows\")\n",
    "    \n",
    "    # Display first few rows to verify structure\n",
    "    print(\"\\nFirst few rows of data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Create an empty directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes and edges to the graph\n",
    "    for _, row in df.iterrows():\n",
    "        source = row['source']\n",
    "        target = row['target']\n",
    "        weight = row['weight']\n",
    "        \n",
    "        # Add nodes if they don't exist\n",
    "        if not G.has_node(source):\n",
    "            G.add_node(source)\n",
    "        if not G.has_node(target):\n",
    "            G.add_node(target)\n",
    "        \n",
    "        # Add edge with weight\n",
    "        G.add_edge(source, target, weight=weight)\n",
    "    \n",
    "    print(f\"Created graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65d4f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "#  Kullback-Leibler divergence \n",
    "#  This function computes K-L between the distributions of link weights and of harvests\n",
    "############################################################################################\n",
    "\n",
    "from scipy.special import rel_entr    \n",
    "\n",
    "def kl_weight_vs_harvest(weight_vec, harvest_vec, bins='fd', eps=1e-12):\n",
    "    \"\"\"\n",
    "    KL( P_weights || Q_harvest )  in bits.\n",
    "    bins – int or 'fd'/'auto'; 40 gives finer resolution than 20.\n",
    "    \"\"\"\n",
    "    edges = np.histogram_bin_edges(np.concatenate([weight_vec,harvest_vec]),\n",
    "                               bins='fd')\n",
    "    if len(edges) < 51:\n",
    "        edges = np.linspace(weight_vec.min(), weight_vec.max(), 51)\n",
    "\n",
    "    pw, _ = np.histogram(weight_vec,  bins=edges); pw = pw / pw.sum()\n",
    "    qh, _ = np.histogram(harvest_vec, bins=edges); qh = qh / qh.sum()\n",
    "    mask  = (pw > 0) & (qh > 0)\n",
    "    if not mask.any():\n",
    "        return np.nan\n",
    "    kl = np.sum(pw[mask] * np.log2(pw[mask] / (qh[mask] + eps)))\n",
    "\n",
    "    if tick in {26,78}:                # choose any harvest-aligned tick\n",
    "        print(\"--- diagnostic tick\", tick)\n",
    "        print(\"bin edges first 6:\", edges[:6])\n",
    "        print(\"pw non-zero bins:\", np.nonzero(pw>0)[0][:10])\n",
    "        print(\"qh non-zero bins:\", np.nonzero(qh>0)[0][:10])\n",
    "\n",
    "    return max(0.0, kl)                # clip tiny negative round-off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d7933f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# Jensen–Shannon distance\n",
    "###########################################################################################\n",
    "\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "def js_weight_vs_harvest(weight_vec, harvest_vec, bins='fd', eps=1e-12):\n",
    "    edges = np.histogram_bin_edges(np.concatenate([weight_vec, harvest_vec]),\n",
    "                                   bins=bins)\n",
    "    pw,_ = np.histogram(weight_vec,  bins=edges); pw = pw / pw.sum()\n",
    "    qh,_ = np.histogram(harvest_vec, bins=edges); qh = qh / qh.sum()\n",
    "    return jensenshannon(pw, qh, base=2.0)   # bits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5247f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# CI for Jensen–Shannon distance\n",
    "###########################################################################################\n",
    "import numpy as np\n",
    "def bootstrap_js(weight_vec, harvest_vec, B=300, bins='fd', max_n=None, seed=0):\n",
    "    \"\"\"\n",
    "    Returns mean, 2.5% and 97.5% quantiles of JS over B bootstrap resamples.\n",
    "    Uses the same preprocessing as your js_weight_vs_harvest().\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # optional downsample to balance sizes (recommended)\n",
    "    if max_n is None:\n",
    "        n = min(len(weight_vec), len(harvest_vec))\n",
    "    else:\n",
    "        n = min(max_n, len(weight_vec), len(harvest_vec))\n",
    "    if n < 10:\n",
    "        return np.nan, np.nan, np.nan\n",
    "\n",
    "    js_vals = []\n",
    "    for _ in range(B):\n",
    "        ws = weight_vec[rng.integers(0, len(weight_vec), n)]\n",
    "        hs = harvest_vec[rng.integers(0, len(harvest_vec), n)]\n",
    "        js_vals.append(js_weight_vs_harvest(ws, hs, bins=bins))\n",
    "    js_vals = np.array(js_vals)\n",
    "    return float(np.nanmean(js_vals)), float(np.nanpercentile(js_vals, 2.5)), float(np.nanpercentile(js_vals, 97.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0807a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Append a single row to CSV, writing header if file doesn't exist/empty\n",
    "############################################################################################\n",
    "import os, csv\n",
    "\n",
    "def append_csv_row(path, columns, values):\n",
    "    \"\"\"Append a single row to CSV, writing header if file doesn't exist/empty.\"\"\"\n",
    "    write_header = (not os.path.exists(path)) or (os.path.getsize(path) == 0)\n",
    "    with open(path, 'a', newline='') as f:\n",
    "        w = csv.writer(f)\n",
    "        if write_header:\n",
    "            w.writerow(columns)\n",
    "        w.writerow(values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decff1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Compute structural complexity metrics for the graph\n",
    "############################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os, glob\n",
    "import pandas as pd\n",
    "from scipy.special import rel_entr  # for relative entropy calculations\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.algorithms import community\n",
    "\n",
    "def compute_structural_complexity(G, harvest_vec, tick, save_dir=None):\n",
    "    \"\"\"\n",
    "    G               : NetworkX graph for weight tick `tick`\n",
    "    harvest_vec     : np.array of 400 harvest values for tick-2    \n",
    "    Compute a set of network‐structural complexity measures:\n",
    "      - degree_entropy: Shannon entropy of the degree distribution (in bits)\n",
    "      - weighted_degree_entropy: entropy of the strength (weighted degree) distribution\n",
    "      - num_communities: number of modules found by greedy modularity\n",
    "      - community_entropy: entropy of the community‐size distribution\n",
    "      - modularity: the partition’s modularity score\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    # 1) Degree‐distribution entropy\n",
    "    degs = np.array([d for _, d in G.degree()]).astype(int)\n",
    "    counts = np.bincount(degs)\n",
    "    probs  = counts[counts > 0] / counts.sum()\n",
    "    metrics['degree_entropy'] = -np.sum(probs * np.log2(probs))\n",
    "    print('degree_entropy,', metrics['degree_entropy'])\n",
    "    with open('degree_entropy_results.csv', 'a') as f:\n",
    "        f.write(f'{tick},{metrics[\"degree_entropy\"]}\\n')\n",
    "\n",
    "   # 2) Strength (weighted degree) entropy – bin continuous strengths into 20 bins\n",
    "    strengths = np.array([d for _, d in G.degree(weight='weight')], dtype=float)\n",
    "    bins      = np.histogram_bin_edges(strengths, bins='fd')  # 'fd' for Freedman-Diaconis rule\n",
    "    cW, _     = np.histogram(strengths, bins=bins)\n",
    "    pW        = cW[cW > 0] / cW.sum()\n",
    "    metrics['weighted_degree_entropy'] = -np.sum(pW * np.log2(pW))\n",
    "    print('weighted_degree_entropy,', metrics['weighted_degree_entropy'])\n",
    "    with open('weighted_degree_entropy_results.csv', 'a') as f:\n",
    "        f.write(f'{tick},{metrics[\"weighted_degree_entropy\"]}\\n') \n",
    "\n",
    "    # 3) Kullback-Leibler divergence and Jensen–Shannon between weights & harvest\n",
    "    if harvest_vec.size:\n",
    "        weights_flat = np.array([d['weight'] for _,_,d in G.edges(data=True)],\n",
    "                            dtype=float)\n",
    "\n",
    "    # --- separate standardisation (shape only) -----------------\n",
    "    if weights_flat.std(ddof=0) == 0 or harvest_vec.std(ddof=0) == 0:\n",
    "    # one of the vectors is constant → divergences undefined\n",
    "        kl_val = np.nan\n",
    "        js_val = np.nan\n",
    "    else:\n",
    "        weights_flat = (weights_flat - weights_flat.mean()) / weights_flat.std(ddof=0)\n",
    "        harvest_vec  = (harvest_vec  - harvest_vec.mean())  / harvest_vec.std(ddof=0)\n",
    "\n",
    "    kl_val = max(0.0, kl_weight_vs_harvest(weights_flat, harvest_vec))\n",
    "    js_val = js_weight_vs_harvest(weights_flat, harvest_vec)\n",
    "\n",
    "    # optional CI (cheap: B=200 and max_n=400)\n",
    "    js_mean, js_lo, js_hi = bootstrap_js(weights_flat, harvest_vec, B=200, bins='fd', max_n=400, seed=42)\n",
    "    metrics['js_boot_mean'] = js_mean\n",
    "    metrics['js_boot_lo']   = js_lo\n",
    "    metrics['js_boot_hi']   = js_hi\n",
    "    # store the results\n",
    "    print('kl_weight_vs_harvest,', kl_val)\n",
    "    metrics['kl_weight_vs_harvest'] = kl_val\n",
    "    metrics['js_weight_vs_harvest'] = js_val\n",
    "\n",
    "    if save_dir:\n",
    "        append_csv_row(os.path.join(save_dir, 'kl_weight_vs_harvest_results.csv'),\n",
    "                    ['tick','kl'], [tick, kl_val])\n",
    "        append_csv_row(os.path.join(save_dir, 'js_weight_vs_harvest_results.csv'),\n",
    "                    ['tick','js'], [tick, js_val])\n",
    "\n",
    "    # NEW: CI table alongside JS\n",
    "        append_csv_row(os.path.join(save_dir, 'js_with_ci_results.csv'),\n",
    "                    ['tick','js','js_lo','js_hi','js_boot_mean'],\n",
    "                    [tick, js_val, js_lo, js_hi, js_mean])\n",
    "    else:\n",
    "        # no harvest → record NaNs so downstream tables stay aligned\n",
    "        metrics['kl_weight_vs_harvest'] = np.nan\n",
    "        metrics['js_weight_vs_harvest'] = np.nan\n",
    "        metrics['js_boot_mean'] = np.nan\n",
    "        metrics['js_boot_lo']   = np.nan\n",
    "        metrics['js_boot_hi']   = np.nan\n",
    "\n",
    "    if save_dir:\n",
    "        append_csv_row(os.path.join(save_dir, 'kl_weight_vs_harvest_results.csv'),\n",
    "                       ['tick','kl'], [tick, ''])\n",
    "        append_csv_row(os.path.join(save_dir, 'js_weight_vs_harvest_results.csv'),\n",
    "                       ['tick','js'], [tick, ''])\n",
    "        append_csv_row(os.path.join(save_dir, 'js_with_ci_results.csv'),\n",
    "                       ['tick','js','js_lo','js_hi','js_boot_mean'],\n",
    "                       [tick, '', '', '', ''])\n",
    "\n",
    "    # 3.2)  entropy of the harvest distribution --------------\n",
    "    if harvest_vec.size > 0:\n",
    "        # use same bin rule as KL so results are comparable\n",
    "        edges = np.histogram_bin_edges(harvest_vec, bins='fd')\n",
    "        ch, _ = np.histogram(harvest_vec, bins=edges)\n",
    "        ph     = ch[ch > 0] / ch.sum()\n",
    "        metrics['harvest_entropy'] = -np.sum(ph * np.log2(ph))\n",
    "    else:\n",
    "        metrics['harvest_entropy'] = np.nan\n",
    "    print('harvest_entropy,', metrics['harvest_entropy'])\n",
    "    with open('harvest_entropy_results.csv', 'a') as f:\n",
    "        f.write(f'{tick},{metrics[\"harvest_entropy\"]}\\n')      \n",
    "   \n",
    "   # 4) Community measures, detection via greedy modularity\n",
    "    #    Work on undirected, weighted graph  \n",
    "    G_undir    = G.to_undirected()\n",
    "    comms      = list(community.greedy_modularity_communities(G_undir, weight='weight'))\n",
    "    sizes      = np.array([len(c) for c in comms])\n",
    "    probs_c    = sizes / sizes.sum()\n",
    "    metrics['num_communities']   = len(comms)\n",
    "    metrics['community_entropy'] = -np.sum(probs_c * np.log2(probs_c))\n",
    "    metrics['modularity']        = community.modularity(G_undir, comms, weight='weight')\n",
    "    print('community_entropy,', metrics['community_entropy'])\n",
    "    with open('community_entropy_results.csv', 'a') as f:\n",
    "        f.write(f'{tick},{metrics[\"community_entropy\"]}\\n')\n",
    "    with open('modularity_results.csv', 'a') as f:\n",
    "        f.write(f'{tick},{metrics[\"modularity\"]}\\n')\n",
    "\n",
    "    # 5) Save the metrics to a CSV file\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99febb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################\n",
    "# Single network analysis\n",
    "# Load the network from the specified CSV file\n",
    "#############################################################################################\n",
    "\n",
    "G = load_network_from_csv(file_path)\n",
    "structural = compute_structural_complexity(G)\n",
    "print(structural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e4338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "### Simulation series network analysis\n",
    "### Create a Data Frame for a time series of structural metrics\n",
    "############################################################################################\n",
    "  \n",
    "def structural_time_series(graphs, harvest_list, ticks, save_dir=None):\n",
    "    \"\"\"\n",
    "    Given a list of (tick, G) pairs, compute struct metrics and plot\n",
    "    their evolution over time.\n",
    "    Return a DataFrame of structural metrics and (optionally) save the plot.\n",
    "    \"\"\"   \n",
    "    records = []\n",
    "    for G, hv, t in zip(graphs, harvest_list, ticks):\n",
    "        m = compute_structural_complexity(G, hv, t, save_dir)\n",
    "        m['tick'] = t\n",
    "        records.append(m)\n",
    "    # ---- build DataFrame from records -------------------\n",
    "    df = pd.DataFrame(records).set_index('tick').sort_index()\n",
    "    print(\"DEBUG columns:\", list(df.columns))      # <— add\n",
    "\n",
    "    # ---- save metrics to CSV file -----------------------------\n",
    "    if save_dir is not None:\n",
    "        csv_name = f\"structural_metrics_{ticks[0]}_{ticks[-1]}.csv\"\n",
    "        df.to_csv(os.path.join(save_dir, csv_name))\n",
    "        print(\"table saved →\", os.path.join(save_dir, csv_name))\n",
    "    # ---- plot metrics evolution -----------------------------\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True)\n",
    "    df['degree_entropy'        ].plot(ax=axes[0,0], marker='o')\n",
    "    df['weighted_degree_entropy'].plot(ax=axes[0,1], marker='o', color='C1')\n",
    "    df['community_entropy'     ].plot(ax=axes[1,0], marker='o', color='C2')\n",
    "    df['num_communities'       ].plot(ax=axes[1,1], marker='o', color='C3')\n",
    "\n",
    "    axes[0,0].set_title(\"Degree entropy\")\n",
    "    axes[0,1].set_title(\"Weighted-degree entropy\")\n",
    "    axes[1,0].set_title(\"Community-size entropy\")\n",
    "    axes[1,1].set_title(\"Number of communities\")\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xlabel(\"Tick\")\n",
    "        ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ---- save if requested -------------------------------------\n",
    "    if save_dir is not None:\n",
    "        fname = f\"structural_metrics_{ticks[0]}_{ticks[-1]}.png\"\n",
    "        fig.savefig(os.path.join(save_dir, fname), dpi=300)\n",
    "        print(\"plot saved →\", os.path.join(save_dir, fname))\n",
    "\n",
    "    # ---- save KL divergence table and plot ---------------------\n",
    "    if save_dir is not None:\n",
    "        # ---- CSV table\n",
    "        kl_csv = os.path.join(save_dir, 'kl_divergence_table.csv')\n",
    "        df[['kl_weight_vs_harvest']].dropna().to_csv(kl_csv)\n",
    "        print(\"KL table saved →\", kl_csv)\n",
    "        \n",
    "    # ---- save JS divergence table and plot ---------------------\n",
    "    if save_dir is not None:\n",
    "        # ---- CSV table\n",
    "        if 'js_weight_vs_harvest' in df.columns:\n",
    "            js_csv = os.path.join(save_dir, 'js_divergence_table.csv')\n",
    "            df[['js_weight_vs_harvest']].dropna().to_csv(js_csv)\n",
    "            ...\n",
    "        else:\n",
    "            print(\"JS column missing — skipping JS table.\")\n",
    "\n",
    " # ---- save the harvest entropy---------------------\n",
    "\n",
    "    if save_dir is not None:\n",
    "        # ---- CSV table\n",
    "        he_csv = os.path.join(save_dir, 'harvest_entropy_results.csv')\n",
    "        df[['harvest_entropy']].dropna().to_csv(he_csv, index=True)\n",
    "        print(\"harvest_entropy table saved →\", he_csv)\n",
    "   \n",
    "# ---- return the DataFrame with all metrics ----------------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535c565",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "### Network Analysis of structural metrics for a Simulation time series \n",
    "### This is the main part of the script that processes multiple CSV files\n",
    "############################################################################################\n",
    "\n",
    "# 1) assume `files` is your sorted list of CSV paths\n",
    "# 2) define a small helper to pull the tick number out of the filename\n",
    "def extract_tick(fp):\n",
    "    # adjust this if your filenames differ\n",
    "    return int(os.path.basename(fp).split('_')[-1].split('.')[0])\n",
    "\n",
    "# 3) build the parallel lists\n",
    "\n",
    "graphs       = []\n",
    "ticks        = []\n",
    "harvest_list = []\n",
    "species_corrs = []\n",
    "\n",
    "for fp in files:\n",
    "    tick = extract_tick(fp)\n",
    "    G    = load_network_from_csv(fp)\n",
    "    for n in G.nodes:\n",
    "        if 'species' not in G.nodes[n]:\n",
    "            # infer from something, or mark as unknown\n",
    "            G.nodes[n]['species'] = -1        # or whatever default you prefer\n",
    "\n",
    "   # ------------------------------------------------------------\n",
    "    # 1. harvest rows for this event (tick-2)\n",
    "    # ------------------------------------------------------------\n",
    "    hdf = harvest_df[harvest_df['tick'] == tick - 2]\n",
    "    if hdf.empty:\n",
    "        continue                           # skip non-harvest ticks\n",
    "\n",
    "    harvest_vec = hdf['harvested'].to_numpy(float)\n",
    "    # ---------- dominant species for each ilex ------------------\n",
    "    tree_cols = [f'species{sp}_neighbors' for sp in range(1, 10)]\n",
    "    hdf['dominant_species'] = (\n",
    "        hdf[tree_cols].idxmax(axis=1).str.extract(r'(\\d+)').astype(int)\n",
    "    )\n",
    "    # ------------------------------------------------------------\n",
    "    # 2. determine dominant neighbour species for each ilex plant\n",
    "    #    (insert these three lines here)\n",
    "    # ------------------------------------------------------------\n",
    "    tree_cols = [f'species{sp}_neighbors' for sp in range(1, 10)]\n",
    "    hdf['dominant_species'] = (\n",
    "        hdf[tree_cols].idxmax(axis=1)          # → \"speciesX_neighbors\"\n",
    "           .str.extract(r'(\\d+)').astype(int)  # keep just the X (1–9)\n",
    "    )\n",
    "    # ------------------------------------------------------------\n",
    "    # 3. per-species aggregates and Pearson correlation\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    # ---------- per-species means --------------------------------\n",
    "    wbar_tick, hbar_tick = [], []\n",
    "    for sp in range(1, 10):\n",
    "        # --- harvest selection first ---------------------------------\n",
    "        h_sel = hdf[hdf['dominant_species'] == sp]['harvested']\n",
    "\n",
    "        # --- edge-weight selection -----------------------------------      \n",
    "        w_edges = [\n",
    "            d['weight']\n",
    "            for u, v, d in G.edges(data=True)\n",
    "            if (G.nodes[u]['species'], G.nodes[v]['species']) in {(sp, 0), (0, sp)}\n",
    "        ]\n",
    "\n",
    "        # If this species has < 3 ilex samples OR no edges, record NaN\n",
    "        if (len(h_sel) < 1) or (not w_edges):\n",
    "            wbar_tick.append(np.nan)\n",
    "            hbar_tick.append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # otherwise store the means\n",
    "        wbar_tick.append(np.mean(w_edges))\n",
    "        hbar_tick.append(h_sel.mean())\n",
    "    # ------------------------------------------------------------\n",
    "        # wbar_tick, hbar_tick grow with NaNs when no data\n",
    "        # ...\n",
    "        w_arr = np.array(wbar_tick, dtype=float)\n",
    "        h_arr = np.array(hbar_tick, dtype=float)\n",
    "        mask  = np.isfinite(w_arr) & np.isfinite(h_arr)\n",
    "\n",
    "        if mask.sum() > 1:\n",
    "            r, _ = stats.pearsonr(w_arr[mask], h_arr[mask])\n",
    "        else:\n",
    "            r = np.nan\n",
    "\n",
    "    species_corrs.append({'tick': tick, 'pearson_w_h': r})\n",
    "\n",
    "    # ---- store for the rest of your pipeline -------------------\n",
    "    graphs.append(G)\n",
    "    ticks.append(tick)\n",
    "    harvest_list.append(harvest_vec)\n",
    "          \n",
    "# ------------------------------------------------------------------\n",
    "#  save Pearson weight–harvest correlations per harvest tick\n",
    "# ------------------------------------------------------------------\n",
    "df_ts       = structural_time_series(graphs, harvest_list, ticks,\n",
    "                                     save_dir=output_dir)\n",
    "\n",
    "df_species  = (pd.DataFrame(species_corrs)\n",
    "                 .set_index('tick')\n",
    "                 .sort_index())\n",
    "print(df_species)\n",
    "\n",
    "print(\"\\nNon-NaN Pearson counts:\",\n",
    "      df_species['pearson_w_h'].notna().sum())\n",
    "\n",
    "# optional CSV for correlation curve\n",
    "df_species.to_csv(os.path.join(output_dir,\n",
    "                               \"pearson_weight_vs_harvest.csv\"))\n",
    "\n",
    "# merge for plotting / joint export\n",
    "df_ts = df_ts.merge(df_species, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# ---- 1. Entropy table --------------------------------------------------\n",
    "\n",
    "entropy_table = df_ts[['weighted_degree_entropy']]          # 1-column DataFrame\n",
    "entropy_csv   = os.path.join(output_dir, 'weighted_degree_entropy.csv')\n",
    "entropy_table.to_csv(entropy_csv)                  # tick,weighted_degree_entropy\n",
    "print(\"\\nENTROPY TABLE\\n\", entropy_table.head())\n",
    "\n",
    "# ---- 2. KL-divergence and JS per plant tables -------------------------------------------\n",
    "\n",
    "kl_table = df_ts[['kl_weight_vs_harvest']].dropna()\n",
    "kl_csv   = os.path.join(output_dir, 'kl_divergence_table.csv')\n",
    "kl_table.to_csv(kl_csv)\n",
    "print(\"\\nKL TABLE\\n\", kl_table.head())\n",
    "\n",
    "js_table = df_ts[['js_weight_vs_harvest']].dropna()\n",
    "\n",
    "if 'js_weight_vs_harvest' in df_ts.columns:\n",
    "    js_csv = os.path.join(save_dir, 'js_divergence_table.csv')\n",
    "    df_ts[['js_weight_vs_harvest']].dropna().to_csv(js_csv)\n",
    "    js_csv   = os.path.join(output_dir, 'js_divergence_table.csv')\n",
    "    js_table.to_csv(js_csv)\n",
    "    print(\"\\nJS TABLE\\n\", js_table.head())\n",
    "else:\n",
    "    print(\"JS column missing — skipping JS table.\")\n",
    "\n",
    "#---- 3. Save all metrics to a CSV file -------------------------------\n",
    "df_ts.to_csv(os.path.join(output_dir, 'all_metrics.csv'))\n",
    "print(\"\\nALL METRICS TABLE\\n\", df_ts.head())\n",
    "\n",
    "# ---- 4. Plotting the time series of structural metrics ----------------\n",
    "# ---- stand-alone figures\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df_ts['weighted_degree_entropy'].dropna().plot(marker='o')\n",
    "plt.title(\"weighted_degree_entropy vs. tick\")\n",
    "plt.xlabel(\"Tick\")\n",
    "plt.ylabel(\"weighted_degree_entropy  [bits]\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df_ts['community_entropy'].dropna().plot(marker='o')\n",
    "plt.title(\"community_entropy vs. tick\")\n",
    "plt.xlabel(\"Tick\")\n",
    "plt.ylabel(\"community_entropy   [bits]\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df_ts['num_communities'].dropna().plot(marker='o')\n",
    "plt.title(\"num_communities vs. tick\")\n",
    "plt.xlabel(\"Tick\")\n",
    "plt.ylabel(\"num_communities  [bits]\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "df_ts['kl_weight_vs_harvest'].dropna().plot(marker='o')\n",
    "plt.title(\"KL(weight‖harvest) vs. tick\"); plt.xlabel(\"Tick\"); plt.ylabel(\"bits\")\n",
    "plt.grid(alpha=0.3)\n",
    "kl_png = os.path.join(output_dir,'kl_timeseries.png')\n",
    "plt.savefig(kl_png,dpi=300); print(\"KL plot saved →\", kl_png)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "df_ts['js_weight_vs_harvest'].dropna().plot(marker='o')\n",
    "plt.title(\"JS(weight‖harvest) vs. tick\"); plt.xlabel(\"Tick\"); plt.ylabel(\"bits\")\n",
    "plt.grid(alpha=0.3)\n",
    "js_png = os.path.join(output_dir,'js_timeseries.png')\n",
    "plt.savefig(js_png,dpi=300); print(\"JS plot saved →\", js_png)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df_ts['harvest_entropy'].dropna().plot(marker='o')\n",
    "plt.title(\"Harvest_Entropy vs. tick\")\n",
    "plt.xlabel(\"Tick\"); plt.ylabel(\"HE  [bits]\")\n",
    "plt.grid(alpha=0.3)\n",
    "he_png = os.path.join(save_dir, 'harvest_entropy.png')\n",
    "plt.savefig(he_png, dpi=300)\n",
    "print(\"HE plot  saved →\", he_png)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "df_ts['pearson_w_h'].dropna().plot(marker='o')\n",
    "plt.title(\"Pearson (weight mean  vs  harvest mean) over time\")\n",
    "plt.xlabel(\"Tick\"); plt.ylabel(\"r\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save tidy JS+CI table\n",
    "if {'js_weight_vs_harvest','js_boot_lo','js_boot_hi'} <= set(df_ts.columns):\n",
    "    js_ci = df_ts[['js_weight_vs_harvest','js_boot_lo','js_boot_hi']].dropna()\n",
    "    js_ci.to_csv(os.path.join(output_dir, 'js_timeseries_with_ci.csv'))\n",
    "\n",
    "    # Standalone figure with CI ribbon\n",
    "    plt.figure(figsize=(6,4))\n",
    "    x = js_ci.index.values\n",
    "    plt.plot(x, js_ci['js_weight_vs_harvest'], marker='o', label='JS')\n",
    "    plt.fill_between(x, js_ci['js_boot_lo'], js_ci['js_boot_hi'], alpha=0.2, label='95% CI')\n",
    "    plt.title('JS(weight‖harvest) with bootstrap CI')\n",
    "    plt.xlabel('Tick'); plt.ylabel('JS [bits]'); plt.grid(alpha=0.3); plt.legend()\n",
    "    out_png = os.path.join(output_dir, 'js_timeseries_with_ci.png')\n",
    "    plt.savefig(out_png, dpi=300, bbox_inches='tight')\n",
    "    print('JS+CI plot saved →', out_png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "443f2094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson_w_h\n",
      "False    9\n",
      "True     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_species['pearson_w_h'].isna().value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09161579",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# --- DIAGNOSE quick post-run diagnosis -------------------\n",
    "############################################################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TEST_TICK = 338                # <- put any weight tick that has harvest (26,78,…)\n",
    "\n",
    "# pull vectors that went into the metric\n",
    "G_test        = graphs[ticks.index(TEST_TICK)]\n",
    "weights_test  = np.array([d['weight'] for _,_,d in G_test.edges(data=True)])\n",
    "harvest_test  = harvest_df.loc[harvest_df['tick']==TEST_TICK-2,'harvested'].to_numpy(float)\n",
    "\n",
    "# same bin rule you used in the metric\n",
    "edges = np.histogram_bin_edges(np.concatenate([weights_test,harvest_test]), bins='fd')\n",
    "pw,_ = np.histogram(weights_test,  bins=edges); pw = pw/pw.sum()\n",
    "qh,_ = np.histogram(harvest_test,  bins=edges);  qh = qh/qh.sum()\n",
    "\n",
    "print(f\"\\nDEBUG  tick {TEST_TICK} (weights)  & {TEST_TICK-2} (harvest)\")\n",
    "print(\"edges min..max:\", edges[0], edges[-1], \"   bins:\", len(edges)-1)\n",
    "print(\"weight mass in bins:\", np.nonzero(pw>0)[0][:10], \" …\")\n",
    "print(\"harvest mass bins:\",    np.nonzero(qh>0)[0][:10], \" …\")\n",
    "\n",
    "# show both histograms\n",
    "fig,ax = plt.subplots(figsize=(6,3))\n",
    "wcent = 0.5*(edges[:-1]+edges[1:])\n",
    "ax.step(wcent, pw, where='mid', label='weights', linewidth=1.5)\n",
    "ax.step(wcent, qh, where='mid', label='harvest', linewidth=1.5)\n",
    "ax.legend(); ax.set(title=f\"Tick {TEST_TICK}: histogram overlay\", ylabel=\"prob.\");\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d51fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# --- DIAGNOSE choose a harvest-aligned tick to inspect -------------\n",
    "############################################################################################\n",
    "TEST_TICK = 338                                    # 338 → harvest 336\n",
    "G_test     = graphs[ticks.index(TEST_TICK)]\n",
    "w_raw      = np.array([d['weight'] for _,_,d in G_test.edges(data=True)],\n",
    "                      dtype=float)\n",
    "h_raw      = harvest_df.loc[harvest_df['tick']==TEST_TICK-2,'harvested'\n",
    "                           ].to_numpy(float)\n",
    "\n",
    "# ---- separate standardisation, just like in the metric ------------\n",
    "if w_raw.std(ddof=0) > 0 and h_raw.std(ddof=0) > 0:\n",
    "    w = (w_raw - w_raw.mean()) / w_raw.std(ddof=0)\n",
    "    h = (h_raw - h_raw.mean()) / h_raw.std(ddof=0)\n",
    "else:\n",
    "    raise ValueError(\"One vector is constant at this tick\")\n",
    "\n",
    "edges = np.histogram_bin_edges(np.concatenate([w, h]), bins='fd')\n",
    "pw,_  = np.histogram(w, bins=edges); pw = pw / pw.sum()\n",
    "qh,_  = np.histogram(h, bins=edges); qh = qh / qh.sum()\n",
    "\n",
    "print(f\"DEBUG  tick {TEST_TICK}:\")\n",
    "print(\"edges min..max:\", edges[0], edges[-1], \" bins:\", len(edges)-1)\n",
    "print(\"w bins with mass:\", np.nonzero(pw>0)[0][:10])\n",
    "print(\"h bins with mass:\", np.nonzero(qh>0)[0][:10])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "cent = 0.5*(edges[:-1]+edges[1:])\n",
    "plt.step(cent, pw, where='mid', label='weights (z)', linewidth=1.4)\n",
    "plt.step(cent, qh, where='mid', label='harvest (z)', linewidth=1.4)\n",
    "plt.legend(); plt.title(f\"Histogram overlay (z-score) – tick {TEST_TICK}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9835ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Full-run diagnostic: overlay histograms for *every* harvest tick\n",
    "# ---------------------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "PLOT_EACH   = True                  # True → pop a window for every tick\n",
    "SAVE_PNGS   = False                   # True → save PNGs in diag_dir\n",
    "diag_dir    = os.path.join(output_dir, \"diagnostic_plots\")\n",
    "os.makedirs(diag_dir, exist_ok=True)\n",
    "\n",
    "for fp in files:                                         # your sorted CSV list\n",
    "    tick = extract_tick(fp)                              # 0, 26, 52, …\n",
    "    hdf  = harvest_df[harvest_df[\"tick\"] == tick - 2]    # harvest tick-2\n",
    "    if hdf.empty:\n",
    "        continue                                         # skip non-harvest ticks\n",
    "\n",
    "    # --- raw vectors ----------------------------------------------\n",
    "    G_test = load_network_from_csv(fp)\n",
    "    w_raw  = np.array([d['weight'] for _,_,d in G_test.edges(data=True)],\n",
    "                      dtype=float)\n",
    "    h_raw  = hdf['harvested'].to_numpy(float)\n",
    "\n",
    "    if w_raw.std(ddof=0) == 0 or h_raw.std(ddof=0) == 0:\n",
    "        print(f\"tick {tick}: one vector is constant; skip\")\n",
    "        continue\n",
    "\n",
    "    # --- same z-score as in metric -------------------------------\n",
    "    w = (w_raw - w_raw.mean()) / w_raw.std(ddof=0)\n",
    "    h = (h_raw - h_raw.mean()) / h_raw.std(ddof=0)\n",
    "\n",
    "    edges = np.histogram_bin_edges(np.concatenate([w, h]), bins='fd')\n",
    "    pw,_ = np.histogram(w, bins=edges); pw = pw / pw.sum()\n",
    "    qh,_ = np.histogram(h, bins=edges); qh = qh / qh.sum()\n",
    "\n",
    "    # quick text summary\n",
    "    print(f\"tick {tick:3d}  bins={len(edges)-1:3d}  overlap bins=\"\n",
    "          f\"{np.sum((pw>0)&(qh>0))}\")\n",
    "\n",
    "    # optional figure\n",
    "    if PLOT_EACH or SAVE_PNGS:\n",
    "        cent = 0.5*(edges[:-1]+edges[1:])\n",
    "        plt.figure(figsize=(5,3))\n",
    "        plt.step(cent, pw, where='mid', label='weights (z)')\n",
    "        plt.step(cent, qh, where='mid', label='harvest (z)')\n",
    "        plt.title(f\"Histogram overlay – weights vs harvest  (tick {tick})\")\n",
    "        plt.xlabel(\"z-score\"); plt.ylabel(\"probability\")\n",
    "        plt.legend(); plt.tight_layout()\n",
    "        if SAVE_PNGS:\n",
    "            fpng = os.path.join(diag_dir, f\"overlay_tick_{tick}.png\")\n",
    "            plt.savefig(fpng, dpi=300)\n",
    "        if PLOT_EACH:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "\n",
    "print(\"\\nDiagnostics complete.  PNGs (if saved) are in:\", diag_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0e741b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Visualize structural metrics for a single graph\n",
    "############################################################################################\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def visualize_structural_one(G, ax_deg=None, ax_str=None, ax_comm=None):\n",
    "    \"\"\"\n",
    "    Given a single graph G, plot:\n",
    "      - unweighted degree hist\n",
    "      - weighted‐degree (strength) hist\n",
    "      - community‐size bar chart\n",
    "    Returns the three Axes so you can plt.tight_layout() on the figure.\n",
    "    \"\"\"\n",
    "    # 1) Degree histogram\n",
    "    degs = np.array([d for _, d in G.degree()])\n",
    "    if ax_deg is None:\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        gs  = fig.add_gridspec(2, 2)\n",
    "        ax_deg  = fig.add_subplot(gs[0, 0])\n",
    "        ax_str  = fig.add_subplot(gs[0, 1])\n",
    "        ax_comm = fig.add_subplot(gs[1, :])\n",
    "    ax_deg.hist(degs, bins=range(degs.min(), degs.max()+2), edgecolor='black', alpha=0.7)\n",
    "    ax_deg.set_title(\"Degree distribution\")\n",
    "    ax_deg.set_xlabel(\"Degree\")\n",
    "    ax_deg.set_ylabel(\"Count\")\n",
    "\n",
    "    # 2) Strength histogram\n",
    "    strengths = np.array([d for _, d in G.degree(weight='weight')])\n",
    "    ax_str.hist(strengths, bins=20, edgecolor='black', alpha=0.7)\n",
    "    ax_str.set_title(\"Strength (weighted degree) distribution\")\n",
    "    ax_str.set_xlabel(\"Strength\")\n",
    "    ax_str.set_ylabel(\"Count\")\n",
    "\n",
    "    # 3) Community sizes\n",
    "    G_undir    = G.to_undirected()\n",
    "    communities = list(community.greedy_modularity_communities(G_undir, weight='weight'))\n",
    "    sizes      = [len(c) for c in communities]\n",
    "    ax_comm.bar(range(len(sizes)), sorted(sizes, reverse=True), color='C2', alpha=0.8)\n",
    "    ax_comm.set_title(\"Community sizes (greedy modularity)\")\n",
    "    ax_comm.set_xlabel(\"Community rank\")\n",
    "    ax_comm.set_ylabel(\"Size (# nodes)\")\n",
    "\n",
    "    return ax_deg, ax_str, ax_comm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "480961f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################################\n",
    "# Visualize structural metrics for a time series of graphs\n",
    "# This function takes a list of graphs and their corresponding ticks\n",
    "############################################################################################\n",
    "def structural_time_series(graphs, ticks):\n",
    "    \"\"\"\n",
    "    Given a list of (tick, G) pairs, compute struct metrics and plot\n",
    "    their evolution over time.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for tick, G in zip(ticks, graphs):\n",
    "        m = compute_structural_complexity(G)\n",
    "        m['tick'] = tick\n",
    "        records.append(m)\n",
    "    df = pd.DataFrame(records).set_index('tick').sort_index()\n",
    "\n",
    "    # plot time series\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 8), sharex=True)\n",
    "    df['degree_entropy'].plot(ax=axes[0,0], marker='o')\n",
    "    axes[0,0].set_title(\"Degree-entropy over time\")\n",
    "    df['weighted_degree_entropy'].plot(ax=axes[0,1], marker='o', color='C1')\n",
    "    axes[0,1].set_title(\"Weighted-degree entropy over time\")\n",
    "    df['community_entropy'].plot(ax=axes[1,0], marker='o', color='C2')\n",
    "    axes[1,0].set_title(\"Community-size entropy over time\")\n",
    "    df['num_communities'].plot(ax=axes[1,1], marker='o', color='C3')\n",
    "    axes[1,1].set_title(\"Number of communities over time\")\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xlabel(\"Tick\")\n",
    "        ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ea804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Example usage for a single G at tick t:\n",
    "# -----------------------\n",
    "for tick, G in zip(ticks, graphs):\n",
    "    print(f\"Visualizing structural metrics for tick {tick}\")\n",
    "ax1, ax2, ax3 = visualize_structural_one(G)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5243a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Example usage across many ticks:\n",
    "# -----------------------\n",
    "# suppose you have\n",
    "#    graphs = [load_network(fp) for fp in files]\n",
    "#    ticks  = [extract_tick(fp) for fp in files]\n",
    "df_ts = structural_time_series(graphs, harvest_list, ticks, save_dir=output_dir)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
